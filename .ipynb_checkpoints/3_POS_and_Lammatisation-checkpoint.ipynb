{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports and Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alessia/anaconda3/lib/python3.6/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "#### Imports and Set Up\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk import pos_tag\n",
    "\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    " \n",
    "porter_stemmer = PorterStemmer()\n",
    "wordnet_lemmatiser = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cwd = os.chdir(\"/Users/alessia/Documents/DataScience/NLP_Project/Data\")\n",
    "cwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cons1_df = pd.read_csv(\"/Users/alessia/Documents/DataScience/NLP_Project/Outputs/cons1_cleantext_SA_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### POS tagging answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get columns' index\n",
    "idx_Q1 = cons1_df.columns.get_loc(str([col for col in cons1_df if 'census methods' in str(col)][0]))\n",
    "idx_Q4 = cons1_df.columns.get_loc(str([col for col in cons1_df if '4. 1. ' in str(col)][0]))\n",
    "idx_Q5 = cons1_df.columns.get_loc(str([col for col in cons1_df if '5. 1.' in str(col)][0]))\n",
    "idx_Q8 = cons1_df.columns.get_loc(str([col for col in cons1_df if '8.' in str(col)][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Q1': 41, 'Q4': 45, 'Q5': 47, 'Q8': 50}\n",
      "dict_items([('Q1', 41), ('Q4', 45), ('Q5', 47), ('Q8', 50)])\n",
      "dict_values([41, 45, 47, 50])\n"
     ]
    }
   ],
   "source": [
    "# Save them in a dictionary\n",
    "col_idx_dict = {\"Q1\":idx_Q1, \"Q4\":idx_Q4, \"Q5\":idx_Q5, \"Q8\":idx_Q8}\n",
    "\n",
    "print(col_idx_dict)\n",
    "print(col_idx_dict.items())\n",
    "print(col_idx_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define function to tag Part-of-Speech of text answers\n",
    "\n",
    "import pandas as pd\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "    \n",
    "\n",
    "def tokenise_POS_text(data, col_ind, stop_words, no_stopwords = True, no_punctuation = True, keep_neg=True) :\n",
    "    \"\"\"Return a list with POS tags of specified data columns containing text after\n",
    "    removing punctuation (default), non-alphabetic tokens, specified stopwords but keeping negations in \"\"\"\n",
    "    \n",
    "    # empty list collector\n",
    "    tokens_bag = []\n",
    "    \n",
    "    for answer in data.iloc[:, col_ind] :   \n",
    "        \n",
    "        # no answer was provided, return NA\n",
    "        if pd.isnull(answer) : \n",
    "            tokens_bag.append(np.nan)\n",
    "            \n",
    "        # an answer was provided    \n",
    "        else : \n",
    "                \n",
    "            # word-tokenise the answer\n",
    "            words = word_tokenize(answer)\n",
    "        \n",
    "            # convert to lower case\n",
    "            words = [w.lower() for w in words]\n",
    "            \n",
    "            \n",
    "            # break words that are of the form word1-word2 into constituting words\n",
    "            \n",
    "            words2 = []\n",
    "            \n",
    "            for w in words :\n",
    "                \n",
    "                if '-' in w :\n",
    "                    \n",
    "                    words2.extend(w.split('-'))\n",
    "                    \n",
    "                else :\n",
    "                    \n",
    "                    words2.append(w)\n",
    "                \n",
    "            \n",
    "            if no_punctuation : # Remove punctuation if no_punctuation = True\n",
    "                \n",
    "                # remove punctuation \n",
    "                table = str.maketrans('', '', string.punctuation)\n",
    "                words = [w.translate(table) for w in words2]\n",
    "                \n",
    "                # remove remaining tokens that are not alphabetic\n",
    "                only_words = [w for w in words2 if w.isalpha()]\n",
    "                \n",
    "                 \n",
    "            \n",
    "            if keep_neg :       # we want to keep in all negations\n",
    "                \n",
    "                for w in only_words :\n",
    "                    \n",
    "                    if w in [\"don't\", \"didn\", \"didn't\", \"doesn\", \"doesn't\", 'hadn', \"hadn't\", 'hasn', \n",
    "                                    \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'mightn', \"mightn't\", \n",
    "                                    'mustn', \"mustn't\", 'needn', \"needn't\", \"shan't\", 'shouldn', \n",
    "                                    \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", \n",
    "                                    'wouldn', \"wouldn't\", 'aren', \"aren't\", 'couldn', \"couldn't\"] :\n",
    "                        w = 'not'\n",
    "                        \n",
    "                    else :\n",
    "                        \n",
    "                        w = w\n",
    "                        \n",
    "                        \n",
    "            if no_stopwords :    # Remove stop-words if no_stopwirds = True\n",
    "                \n",
    "                # filter out stop words from each answer\n",
    "                only_words = [w for w in only_words if not w in stop_words]\n",
    "                \n",
    "           \n",
    "            \n",
    "            # calculate Part-Of-Speech\n",
    "            pos_answer = pos_tag(only_words)\n",
    "\n",
    "            tokens_bag.append(pos_answer)\n",
    "    \n",
    "    return(tokens_bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#batch_pos_tag(cons1_df['Q1'])   # ERROR name 'batch_pos_tag' is not defined "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stopwords.words('english');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Customise list of stop words\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# Let's exclude (i.e., we want to keep them in the text):\n",
    "# negations\n",
    "# only, up, down, and other opinion-realted words\n",
    "\n",
    "\n",
    "stop_words1 = [w for w in stop_words if not w in ['no', 'nor', 'not', 'only', 'up', 'down', 'further', 'too', 'against']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create new dataset columns containing the POS-tagged texts\n",
    "\n",
    "for q, idx in col_idx_dict.items() :\n",
    "\n",
    "    result = tokenise_POS_text(cons1_df, idx, stop_words=stop_words1)\n",
    "    new_q = q + '_pos'\n",
    "    \n",
    "    #print(type(result))\n",
    "    #print(type(cons1_df.iloc[:, idx]))\n",
    "    #print(type(new_q))\n",
    "    \n",
    "    se_result = pd.Series(result)      # had to turn this into a Pandas series first, otherwise ERROR\n",
    "    #print(se_result[1:5])\n",
    "    #print(cons1_df.iloc[:, idx].head())\n",
    "    \n",
    "    cons1_df.loc[:, new_q] = se_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Checks\n",
    "pd.set_option('display.max_colwidth', -1);\n",
    "cons1_df.iloc[41:42, [idx_Q1, -8]];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatisation of answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get columns' index of POS-tagged answers\n",
    "idx_Q1p = cons1_df.columns.get_loc('Q1_pos')\n",
    "idx_Q4p = cons1_df.columns.get_loc('Q4_pos')\n",
    "idx_Q5p = cons1_df.columns.get_loc('Q5_pos')\n",
    "idx_Q8p = cons1_df.columns.get_loc('Q8_pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save them in a dictionary\n",
    "colpos_idx_dict = {\"Q1_pos\":idx_Q1p, \"Q4_pos\":idx_Q4p, \"Q5_pos\":idx_Q5p, \"Q8_pos\":idx_Q8p}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replace float nan's with empty srings\n",
    "\n",
    "# Create new dataset columns containing the POS-tagged texts\n",
    "\n",
    "for q, idx in colpos_idx_dict.items() :\n",
    "    \n",
    "    cons1_df.iloc[:, idx] = cons1_df.iloc[:, idx].replace(np.nan, '', regex=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(cons1_df.iloc[:, idx].head(4));\n",
    "cons1_df.iloc[:,].head(4);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TBC : should impement something like this...\n",
    "# https://stackoverflow.com/questions/15586721/wordnet-lemmatization-and-pos-tagging-in-python\n",
    "\n",
    "# The following function would map the treebank tags to WordNet part of speech names:\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    elif treebank_tag.startswith('S'):\n",
    "        return wordnet.ADJ_SAT\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_wordnet_pos('MD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create new dataset columns containing the POS-tagged texts\n",
    "\n",
    "for q, idx in colpos_idx_dict.items() :\n",
    "\n",
    "    # set new variable name\n",
    "    new_q = q + '_lemma'\n",
    "    \n",
    "    \n",
    "    # extract columns with answers\n",
    "    answer_col = cons1_df.iloc[:, idx]\n",
    "    \n",
    "    answer_col = answer_col.tolist()\n",
    "    #print(type(answer_col))\n",
    "    #print(answer_col[1:4])\n",
    "    \n",
    "    \n",
    "    # collector for all answers within that answer_col\n",
    "    lemma_big_bag = []\n",
    "    \n",
    "    \n",
    "    for answer in answer_col :\n",
    "        \n",
    "        lemma_bag = []\n",
    "        \n",
    "        #print(answer)\n",
    "        #print(len(answer))\n",
    "        \n",
    "        # an answer was provided\n",
    "        if len(answer) > 0 :\n",
    "            \n",
    "            for POStext_pair in answer :\n",
    "                \n",
    "                #print(POStext_pair[0])\n",
    "                #print(POStext_pair[1])\n",
    "                \n",
    "                #print(type(POStext_pair[0]))\n",
    "                #print(type(POStext_pair[1]))\n",
    "                \n",
    "                #print('wordnet pos = ' + get_wordnet_pos(POStext_pair[1]))\n",
    "                #print('type wordnet pos = ' + str(type(get_wordnet_pos(POStext_pair[1]))))\n",
    "                \n",
    "                #print( get_wordnet_pos(POStext_pair[1]) == '')\n",
    "                \n",
    "                \n",
    "                # the treebank POS does not have a wordnet POS equivalent\n",
    "                if get_wordnet_pos(POStext_pair[1]) == '' :\n",
    "                    \n",
    "                    lemma = POStext_pair[0]\n",
    "                    #print('lemma = ' + lemma)\n",
    "                    #print(type(lemma))\n",
    "                    \n",
    "                \n",
    "                \n",
    "                # the treebank POS does have a wordnet POS equivalent\n",
    "                else :\n",
    "                    \n",
    "                    lemma = wordnet_lemmatiser.lemmatize(POStext_pair[0], pos=get_wordnet_pos(POStext_pair[1]))\n",
    "                \n",
    "                    #print('lemma = ' + lemma)\n",
    "                    #print(type(lemma))\n",
    "                    \n",
    "                \n",
    "                lemma_bag.append(lemma)\n",
    "                #print(lemma_bag)\n",
    "                #print(type(lemma_bag))\n",
    "                \n",
    "        else :\n",
    "            \n",
    "            lemma_bag.append(str(\"\"))\n",
    "        \n",
    "        \n",
    "        lemma_big_bag.append(lemma_bag)\n",
    "        \n",
    "    \n",
    "    \n",
    "    se_lemma_result = pd.Series(lemma_big_bag)      # had to turn this into a Pandas series first, otherwise ERROR\n",
    "    \n",
    "    cons1_df.loc[:, new_q] = se_lemma_result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "cons1_df.head(4);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
