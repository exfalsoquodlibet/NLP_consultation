{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports and Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Imports and Set Up\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk import pos_tag\n",
    "\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    " \n",
    "porter_stemmer = PorterStemmer()\n",
    "lemmatiser = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cwd = os.chdir(\"/Users/alessia/Documents/DataScience/NLP_Project/Data\")\n",
    "cwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cons1_df = pd.read_csv(\"/Users/alessia/Documents/DataScience/NLP_Project/Outputs/cons1_cleantext_SA_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get columns' index\n",
    "idx_Q1 = cons1_df.columns.get_loc(str([col for col in cons1_df if 'census methods' in str(col)][0]))\n",
    "idx_Q4 = cons1_df.columns.get_loc(str([col for col in cons1_df if '4. 1. ' in str(col)][0]))\n",
    "idx_Q5 = cons1_df.columns.get_loc(str([col for col in cons1_df if '5. 1.' in str(col)][0]))\n",
    "idx_Q8 = cons1_df.columns.get_loc(str([col for col in cons1_df if '8.' in str(col)][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Q1': 41, 'Q4': 45, 'Q5': 47, 'Q8': 50}\n",
      "dict_items([('Q1', 41), ('Q4', 45), ('Q5', 47), ('Q8', 50)])\n",
      "dict_values([41, 45, 47, 50])\n"
     ]
    }
   ],
   "source": [
    "# Save them in a dictionary\n",
    "col_idx_dict = {\"Q1\":idx_Q1, \"Q4\":idx_Q4, \"Q5\":idx_Q5, \"Q8\":idx_Q8}\n",
    "\n",
    "print(col_idx_dict)\n",
    "print(col_idx_dict.items())\n",
    "print(col_idx_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to clean answer text\n",
    "\n",
    "def tokenise_POS_text(data, col_ind, no_stopwords = False, no_punctuation = True) :\n",
    "    \"\"\"Return a list with POS tags of specified data columns containing text after\n",
    "    removing punctuation (default) and non-alphabetic tokens\"\"\"\n",
    "    \n",
    "    import pandas as pd\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    \n",
    "    from nltk import pos_tag\n",
    "    \n",
    "    # empty list collector\n",
    "    tokens_bag = []\n",
    "    \n",
    "    for answer in data.iloc[:, col_ind] :   \n",
    "        \n",
    "        # no answer was provided, return NA\n",
    "        if pd.isnull(answer) : \n",
    "            tokens_bag.append(np.nan)\n",
    "            \n",
    "        # an answer was provided    \n",
    "        else : \n",
    "                \n",
    "            # word-tokenise the answer\n",
    "            words = word_tokenize(answer)\n",
    "        \n",
    "            # convert to lower case\n",
    "            words = [w.lower() for w in words]\n",
    "            \n",
    "            \n",
    "            if no_punctuation : # no_punctuation = True\n",
    "                \n",
    "                # remove punctuation \n",
    "                import string\n",
    "                table = str.maketrans('', '', string.punctuation)\n",
    "                words = [w.translate(table) for w in words]\n",
    "                \n",
    "                # remove remaining tokens that are not alphabetic\n",
    "                only_words = [w for w in words if w.isalpha()]\n",
    "                \n",
    "            else :\n",
    "                \n",
    "                continue\n",
    "                \n",
    "            \n",
    "            ### THIS DOES NOT WORK... ###\n",
    "            #if no_stopwords :    # no_stopwirds = True\n",
    "                \n",
    "                # filter out stop words from each answer\n",
    "            #    only_words = [w for w in only_words if not w in stop_words]\n",
    "                \n",
    "            #else :\n",
    "                \n",
    "            #    continue\n",
    "            \n",
    "            \n",
    "            # calculate Part-Of-Speech\n",
    "            pos_answer = pos_tag(only_words)\n",
    "            \n",
    "                \n",
    "            # untokenise the sentence: return one unique string for each answer in prep for sentiment analysis\n",
    "            #from nltk.tokenize.moses import MosesDetokenizer\n",
    "            #detokenizer = MosesDetokenizer()\n",
    "        \n",
    "            #filtered_answer = detokenizer.detokenize(pos_answer, return_str=True)\n",
    "\n",
    "            tokens_bag.append(pos_answer)\n",
    "    \n",
    "    return(tokens_bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch_pos_tag(cons1_df['Q1'])   # ERROR name 'batch_pos_tag' is not defined "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenise_POS_text(cons1_df.iloc[:, ], col_ind=idx_Q8);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new dataset columns containing the POS-tagged texts\n",
    "\n",
    "for q, idx in col_idx_dict.items() :\n",
    "\n",
    "    result = tokenise_POS_text(cons1_df, idx)\n",
    "    new_q = q + '_pos'\n",
    "    \n",
    "    #print(type(result))\n",
    "    #print(type(cons1_df.iloc[:, idx]))\n",
    "    #print(type(new_q))\n",
    "    \n",
    "    se_result = pd.Series(result)      # had to turn this into a Pandas series first, otherwise ERROR\n",
    "    #print(se_result[1:5])\n",
    "    #print(cons1_df.iloc[:, idx].head())\n",
    "    \n",
    "    cons1_df.loc[:, new_q] = se_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Respondent ID</th>\n",
       "      <th>Collector ID</th>\n",
       "      <th>Start Date</th>\n",
       "      <th>End Date</th>\n",
       "      <th>IP Address</th>\n",
       "      <th>Email Address</th>\n",
       "      <th>First Name</th>\n",
       "      <th>Last Name</th>\n",
       "      <th>...</th>\n",
       "      <th>Q5_clean</th>\n",
       "      <th>Q8_clean</th>\n",
       "      <th>Q1_cl_sentiment</th>\n",
       "      <th>Q4_cl_sentiment</th>\n",
       "      <th>Q5_cl_sentiment</th>\n",
       "      <th>Q8_cl_sentiment</th>\n",
       "      <th>Q1_pos</th>\n",
       "      <th>Q4_pos</th>\n",
       "      <th>Q5_pos</th>\n",
       "      <th>Q8_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3001215611</td>\n",
       "      <td>45151668</td>\n",
       "      <td>2014-01-05 02:42:21</td>\n",
       "      <td>2014-01-05 02:44:13</td>\n",
       "      <td>49.224.154.245</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3001062135</td>\n",
       "      <td>45151668</td>\n",
       "      <td>2014-01-04 21:34:56</td>\n",
       "      <td>2014-01-04 21:35:12</td>\n",
       "      <td>79.69.231.100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2990699680</td>\n",
       "      <td>45151668</td>\n",
       "      <td>2013-12-23 16:54:29</td>\n",
       "      <td>2013-12-23 17:00:18</td>\n",
       "      <td>109.148.186.17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2990403881</td>\n",
       "      <td>45151668</td>\n",
       "      <td>2013-12-23 12:17:33</td>\n",
       "      <td>2013-12-23 12:29:22</td>\n",
       "      <td>217.36.37.20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>date statistics postcode sector equivalent lev...</td>\n",
       "      <td>essential changes census methodology thoroughl...</td>\n",
       "      <td>0.5719</td>\n",
       "      <td>0.6486</td>\n",
       "      <td>0.4404</td>\n",
       "      <td>0.8910</td>\n",
       "      <td>[(moving, VBG), (to, TO), (a, DT), (primarily,...</td>\n",
       "      <td>[(it, PRP), (is, VBZ), (important, JJ), (for, ...</td>\n",
       "      <td>[(up, RB), (to, TO), (date, NN), (statistics, ...</td>\n",
       "      <td>[(it, PRP), (is, VBZ), (essential, JJ), (that,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2985513376</td>\n",
       "      <td>45151668</td>\n",
       "      <td>2013-12-19 11:35:42</td>\n",
       "      <td>2013-12-19 11:43:35</td>\n",
       "      <td>86.12.129.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>would allow council respond effectively changi...</td>\n",
       "      <td>measures must put place ensure one excluded on...</td>\n",
       "      <td>0.9848</td>\n",
       "      <td>0.8360</td>\n",
       "      <td>0.9590</td>\n",
       "      <td>0.4939</td>\n",
       "      <td>[(a, DT), (regular, JJ), (full, JJ), (populati...</td>\n",
       "      <td>[(would, MD), (lose, VB), (the, DT), (ability,...</td>\n",
       "      <td>[(it, PRP), (would, MD), (allow, VB), (the, DT...</td>\n",
       "      <td>[(measures, NNS), (must, MD), (be, VB), (put, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2983385436</td>\n",
       "      <td>45151668</td>\n",
       "      <td>2013-12-18 11:07:44</td>\n",
       "      <td>2013-12-18 16:42:33</td>\n",
       "      <td>46.33.158.20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>users census place premium current model howev...</td>\n",
       "      <td>0.9648</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9657</td>\n",
       "      <td>[(privacy, NN), (is, VBZ), (a, DT), (clear, JJ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[(there, EX), (are, VBP), (some, DT), (users, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 68 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1  Respondent ID  Collector ID           Start Date  \\\n",
       "0           0             0     3001215611      45151668  2014-01-05 02:42:21   \n",
       "1           1             1     3001062135      45151668  2014-01-04 21:34:56   \n",
       "2           2             2     2990699680      45151668  2013-12-23 16:54:29   \n",
       "3           3             3     2990403881      45151668  2013-12-23 12:17:33   \n",
       "4           4             4     2985513376      45151668  2013-12-19 11:35:42   \n",
       "5           5             5     2983385436      45151668  2013-12-18 11:07:44   \n",
       "\n",
       "              End Date      IP Address  Email Address  First Name  Last Name  \\\n",
       "0  2014-01-05 02:44:13  49.224.154.245            NaN         NaN        NaN   \n",
       "1  2014-01-04 21:35:12   79.69.231.100            NaN         NaN        NaN   \n",
       "2  2013-12-23 17:00:18  109.148.186.17            NaN         NaN        NaN   \n",
       "3  2013-12-23 12:29:22    217.36.37.20            NaN         NaN        NaN   \n",
       "4  2013-12-19 11:43:35     86.12.129.3            NaN         NaN        NaN   \n",
       "5  2013-12-18 16:42:33    46.33.158.20            NaN         NaN        NaN   \n",
       "\n",
       "                         ...                          \\\n",
       "0                        ...                           \n",
       "1                        ...                           \n",
       "2                        ...                           \n",
       "3                        ...                           \n",
       "4                        ...                           \n",
       "5                        ...                           \n",
       "\n",
       "                                            Q5_clean  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3  date statistics postcode sector equivalent lev...   \n",
       "4  would allow council respond effectively changi...   \n",
       "5                                                NaN   \n",
       "\n",
       "                                            Q8_clean Q1_cl_sentiment  \\\n",
       "0                                                NaN             NaN   \n",
       "1                                                NaN             NaN   \n",
       "2                                                NaN             NaN   \n",
       "3  essential changes census methodology thoroughl...          0.5719   \n",
       "4  measures must put place ensure one excluded on...          0.9848   \n",
       "5  users census place premium current model howev...          0.9648   \n",
       "\n",
       "  Q4_cl_sentiment Q5_cl_sentiment Q8_cl_sentiment  \\\n",
       "0             NaN             NaN             NaN   \n",
       "1             NaN             NaN             NaN   \n",
       "2             NaN             NaN             NaN   \n",
       "3          0.6486          0.4404          0.8910   \n",
       "4          0.8360          0.9590          0.4939   \n",
       "5             NaN             NaN          0.9657   \n",
       "\n",
       "                                              Q1_pos  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3  [(moving, VBG), (to, TO), (a, DT), (primarily,...   \n",
       "4  [(a, DT), (regular, JJ), (full, JJ), (populati...   \n",
       "5  [(privacy, NN), (is, VBZ), (a, DT), (clear, JJ...   \n",
       "\n",
       "                                              Q4_pos  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3  [(it, PRP), (is, VBZ), (important, JJ), (for, ...   \n",
       "4  [(would, MD), (lose, VB), (the, DT), (ability,...   \n",
       "5                                                NaN   \n",
       "\n",
       "                                              Q5_pos  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3  [(up, RB), (to, TO), (date, NN), (statistics, ...   \n",
       "4  [(it, PRP), (would, MD), (allow, VB), (the, DT...   \n",
       "5                                                NaN   \n",
       "\n",
       "                                              Q8_pos  \n",
       "0                                                NaN  \n",
       "1                                                NaN  \n",
       "2                                                NaN  \n",
       "3  [(it, PRP), (is, VBZ), (essential, JJ), (that,...  \n",
       "4  [(measures, NNS), (must, MD), (be, VB), (put, ...  \n",
       "5  [(there, EX), (are, VBP), (some, DT), (users, ...  \n",
       "\n",
       "[6 rows x 68 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checks\n",
    "cons1_df.columns.values\n",
    "cons1_df.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
