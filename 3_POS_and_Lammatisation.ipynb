{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### INTRO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this script, we POS tag and lemmatise the answers to the consultation. Steps:\n",
    "\n",
    "1. Text cleaning: remove punctuation, non-alphabetic tokens, and specified stopwords (from the set of stopwords we exclude negations ('no', 'nor', 'not') and other words that may be used to express opnions ('only', 'up', 'down', 'further', 'too', 'against').\n",
    "\n",
    "2. We replace nagation forms of auxaliary and modal verbs (by default contained in the stop words set for English) with negation\n",
    "\n",
    "3. We POS tag the answer using Treebank POS\n",
    "\n",
    "4. We translate Treebank POS tags into WorldNet POS tags\n",
    "\n",
    "5. We lemmatise the answers using WorldNet lemmatizer \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports and Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alessia/anaconda3/lib/python3.6/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "#### Imports and Set Up\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk import pos_tag\n",
    "\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    " \n",
    "porter_stemmer = PorterStemmer()\n",
    "wordnet_lemmatiser = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cwd = os.chdir(\"/Users/alessia/Documents/DataScience/NLP_Project/Data\")\n",
    "cwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cons1_df = pd.read_csv(\"/Users/alessia/Documents/DataScience/NLP_Project/Outputs/cons1_cleantext_SA_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### POS tagging answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get columns' index\n",
    "idx_Q1 = cons1_df.columns.get_loc(str([col for col in cons1_df if 'census methods' in str(col)][0]))\n",
    "idx_Q4 = cons1_df.columns.get_loc(str([col for col in cons1_df if '4. 1. ' in str(col)][0]))\n",
    "idx_Q5 = cons1_df.columns.get_loc(str([col for col in cons1_df if '5. 1.' in str(col)][0]))\n",
    "idx_Q8 = cons1_df.columns.get_loc(str([col for col in cons1_df if '8.' in str(col)][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Q1': 41, 'Q4': 45, 'Q5': 47, 'Q8': 50}\n",
      "dict_items([('Q1', 41), ('Q4', 45), ('Q5', 47), ('Q8', 50)])\n",
      "dict_values([41, 45, 47, 50])\n"
     ]
    }
   ],
   "source": [
    "# Save them in a dictionary\n",
    "col_idx_dict = {\"Q1\":idx_Q1, \"Q4\":idx_Q4, \"Q5\":idx_Q5, \"Q8\":idx_Q8}\n",
    "\n",
    "print(col_idx_dict)\n",
    "print(col_idx_dict.items())\n",
    "print(col_idx_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define function to tag Part-of-Speech of text answers\n",
    "\n",
    "import pandas as pd\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "    \n",
    "\n",
    "def tokenise_POS_text(data, col_ind, stop_words, no_stopwords = True, no_punctuation = True, keep_neg=True) :\n",
    "    \"\"\"Return a list with POS tags of specified data columns containing text after\n",
    "    removing punctuation (default), non-alphabetic tokens, specified stopwords but keeping negations in \"\"\"\n",
    "    \n",
    "    # empty list collector\n",
    "    tokens_bag = []\n",
    "    \n",
    "    for answer in data.iloc[:, col_ind] :   \n",
    "        \n",
    "        # no answer was provided, return NA\n",
    "        if pd.isnull(answer) : \n",
    "            tokens_bag.append(np.nan)\n",
    "            \n",
    "        # an answer was provided    \n",
    "        else : \n",
    "                \n",
    "            # 1. word-tokenise the answer\n",
    "            words = word_tokenize(answer)\n",
    "        \n",
    "            # 2. convert to lower case\n",
    "            words = [w.lower() for w in words]\n",
    "            \n",
    "            \n",
    "            # 3. break words that are of the form word1-word2 into constituting words\n",
    "            \n",
    "            words2 = []\n",
    "            \n",
    "            for w in words :\n",
    "                \n",
    "                if '-' in w :\n",
    "                    \n",
    "                    words2.extend(w.split('-'))\n",
    "                    \n",
    "                else :\n",
    "                    \n",
    "                    words2.append(w)\n",
    "                \n",
    "            \n",
    "            if no_punctuation : # Remove punctuation if no_punctuation = True\n",
    "                \n",
    "                # 4. remove punctuation \n",
    "                table = str.maketrans('', '', string.punctuation)\n",
    "                words = [w.translate(table) for w in words2]\n",
    "                \n",
    "                # 5. remove remaining tokens that are not alphabetic\n",
    "                only_words = [w for w in words2 if w.isalpha()]\n",
    "                \n",
    "                 \n",
    "            \n",
    "            if keep_neg :       # we want to keep in all negations\n",
    "                \n",
    "                for w in only_words :\n",
    "                    \n",
    "                    if w in [\"don't\", \"didn\", \"didn't\", \"doesn\", \"doesn't\", 'hadn', \"hadn't\", 'hasn', \n",
    "                             \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'mightn', \"mightn't\", \n",
    "                             'mustn', \"mustn't\", 'needn', \"needn't\", \"shan't\", 'shouldn', \n",
    "                             \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", \n",
    "                             'wouldn', \"wouldn't\", 'aren', \"aren't\", 'couldn', \"couldn't\"] :\n",
    "                        \n",
    "                        w = 'not'\n",
    "                        \n",
    "                    else :\n",
    "                        \n",
    "                        w = w\n",
    "                        \n",
    "                        \n",
    "            if no_stopwords :    # Remove stop-words if no_stopwirds = True\n",
    "                \n",
    "                # filter out stop words from each answer\n",
    "                only_words = [w for w in only_words if not w in stop_words]\n",
    "                \n",
    "           \n",
    "            \n",
    "            # calculate Part-Of-Speech\n",
    "            pos_answer = pos_tag(only_words)\n",
    "\n",
    "            tokens_bag.append(pos_answer)\n",
    "    \n",
    "    return(tokens_bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#batch_pos_tag(cons1_df['Q1'])   # ERROR name 'batch_pos_tag' is not defined "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stopwords.words('english');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Customise list of stop words\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# Let's exclude (i.e., we want to keep them in the text):\n",
    "# negations\n",
    "# only, up, down, and other opinion-realted words\n",
    "\n",
    "\n",
    "stop_words1 = [w for w in stop_words if not w in ['no', 'nor', 'not', 'only', 'up', 'down', 'further', 'too', 'against']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create new dataset columns containing the POS-tagged texts\n",
    "\n",
    "for q, idx in col_idx_dict.items() :\n",
    "\n",
    "    result = tokenise_POS_text(cons1_df, idx, stop_words=stop_words1)\n",
    "    new_q = q + '_pos'\n",
    "    \n",
    "    #print(type(result))\n",
    "    #print(type(cons1_df.iloc[:, idx]))\n",
    "    #print(type(new_q))\n",
    "    \n",
    "    se_result = pd.Series(result)      # had to turn this into a Pandas series first, otherwise ERROR\n",
    "    #print(se_result[1:5])\n",
    "    #print(cons1_df.iloc[:, idx].head())\n",
    "    \n",
    "    cons1_df.loc[:, new_q] = se_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Checks\n",
    "pd.set_option('display.max_colwidth', -1);\n",
    "cons1_df.iloc[41:42, [idx_Q1, -8]];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatisation of answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get columns' index of POS-tagged answers\n",
    "idx_Q1p = cons1_df.columns.get_loc('Q1_pos')\n",
    "idx_Q4p = cons1_df.columns.get_loc('Q4_pos')\n",
    "idx_Q5p = cons1_df.columns.get_loc('Q5_pos')\n",
    "idx_Q8p = cons1_df.columns.get_loc('Q8_pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save them in a dictionary\n",
    "colpos_idx_dict = {\"Q1_pos\":idx_Q1p, \"Q4_pos\":idx_Q4p, \"Q5_pos\":idx_Q5p, \"Q8_pos\":idx_Q8p}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replace float nan's with empty srings\n",
    "\n",
    "# Create new dataset columns containing the POS-tagged texts\n",
    "\n",
    "for q, idx in colpos_idx_dict.items() :\n",
    "    \n",
    "    cons1_df.iloc[:, idx] = cons1_df.iloc[:, idx].replace(np.nan, '', regex=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type(cons1_df.iloc[:, idx].head(4));\n",
    "cons1_df.iloc[:,].head(4);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TBC : should impement something like this...\n",
    "# https://stackoverflow.com/questions/15586721/wordnet-lemmatization-and-pos-tagging-in-python\n",
    "\n",
    "# The following function would map the treebank tags to WordNet part of speech names:\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    elif treebank_tag.startswith('S'):\n",
    "        return wordnet.ADJ_SAT\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_wordnet_pos('MD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create new dataset columns containing the POS-tagged texts\n",
    "\n",
    "for q, idx in colpos_idx_dict.items() :\n",
    "\n",
    "    # set new variable name\n",
    "    new_q = q + '_lemma'\n",
    "    \n",
    "    \n",
    "    # extract columns with answers\n",
    "    answer_col = cons1_df.iloc[:, idx]\n",
    "    \n",
    "    answer_col = answer_col.tolist()\n",
    "    #print(type(answer_col))\n",
    "    #print(answer_col[1:4])\n",
    "    \n",
    "    \n",
    "    # collector for all answers within that answer_col\n",
    "    lemma_big_bag = []\n",
    "    \n",
    "    \n",
    "    for answer in answer_col :\n",
    "        \n",
    "        lemma_bag = []\n",
    "        \n",
    "        #print(answer)\n",
    "        #print(len(answer))\n",
    "        \n",
    "        # an answer was provided\n",
    "        if len(answer) > 0 :\n",
    "            \n",
    "            for POStext_pair in answer :\n",
    "                \n",
    "                #print(POStext_pair[0])\n",
    "                #print(POStext_pair[1])\n",
    "                \n",
    "                #print(type(POStext_pair[0]))\n",
    "                #print(type(POStext_pair[1]))\n",
    "                \n",
    "                #print('wordnet pos = ' + get_wordnet_pos(POStext_pair[1]))\n",
    "                #print('type wordnet pos = ' + str(type(get_wordnet_pos(POStext_pair[1]))))\n",
    "                \n",
    "                #print( get_wordnet_pos(POStext_pair[1]) == '')\n",
    "                \n",
    "                \n",
    "                # the treebank POS does not have a wordnet POS equivalent\n",
    "                if get_wordnet_pos(POStext_pair[1]) == '' :\n",
    "                    \n",
    "                    lemma = POStext_pair[0]\n",
    "                    #print('lemma = ' + lemma)\n",
    "                    #print(type(lemma))\n",
    "                    \n",
    "                \n",
    "                \n",
    "                # the treebank POS does have a wordnet POS equivalent\n",
    "                else :\n",
    "                    \n",
    "                    lemma = wordnet_lemmatiser.lemmatize(POStext_pair[0], pos=get_wordnet_pos(POStext_pair[1]))\n",
    "                \n",
    "                    #print('lemma = ' + lemma)\n",
    "                    #print(type(lemma))\n",
    "                    \n",
    "                \n",
    "                lemma_bag.append(lemma)\n",
    "                #print(lemma_bag)\n",
    "                #print(type(lemma_bag))\n",
    "                \n",
    "        else :\n",
    "            \n",
    "            lemma_bag.append(str(\"\"))\n",
    "        \n",
    "        \n",
    "        lemma_big_bag.append(lemma_bag)\n",
    "        \n",
    "    \n",
    "    \n",
    "    se_lemma_result = pd.Series(lemma_big_bag)      # had to turn this into a Pandas series first, otherwise ERROR\n",
    "    \n",
    "    cons1_df.loc[:, new_q] = se_lemma_result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    []                                                                                                                                                                                                                                                                                           \n",
       "1    []                                                                                                                                                                                                                                                                                           \n",
       "2    []                                                                                                                                                                                                                                                                                           \n",
       "3    [move, primarily, online, census, inevitable, necessary, evolution, exist, approach, admin, data, survey, unknown, quantity, dependent, quality, admin, data, not, clear, well, would, fulfil, primary, aim, census, produce, accurate, independent, estimate, size, composition, population]\n",
       "Name: Q1_pos_lemma, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cons1_df.Q1_pos_lemma.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save dataset\n",
    "\n",
    "cons1_df.to_csv('/Users/alessia/Documents/DataScience/NLP_Project/Outputs/cons1_lemmas_df.csv', encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
