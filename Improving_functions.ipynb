{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to sentence-tokenise answers \n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "def sent_tokenise_answer(answer_col) :\n",
    "    \n",
    "    \"\"\" Function to sentence-tokenise answers. Return a list of lists that contain sentences as strings.\n",
    "        \n",
    "        data = name of the dataframe\n",
    "        col_ind = index of the column that contains the texts to be sentence-tokenised\n",
    "    \"\"\"\n",
    "    \n",
    "    sents_collector = []\n",
    "    \n",
    "    for answer in answer_col :\n",
    "        \n",
    "        # no answer was provided -> return empty string list\n",
    "        if pd.isnull(answer) :\n",
    "            sents_collector.append(list(\"\"))\n",
    "            \n",
    "        # an answer was provided    \n",
    "        else :\n",
    "            sents_collector.append(sent_tokenize(answer))\n",
    "            \n",
    "    return(sents_collector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cons1_df = pd.read_csv(\"/Users/alessia/Documents/DataScience/NLP_Project/Outputs/cons1_lemmas_df.csv\", nrows=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get column index of questions\n",
    "idx_Q1 = cons1_df.columns.get_loc(str([col for col in cons1_df if 'census methods' in str(col)][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_Q1     #42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to word-tokenise sentences \n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def word_tokenise_answer(answer_col) :\n",
    "    \n",
    "    \"\"\" \n",
    "    Function to word-tokenise answers' sentences. \n",
    "    Return a list of lists of lower-case words as strings. \n",
    "    Required input, a list of lists containing sentences as strings.\n",
    "        \n",
    "    answer_col = name of the dataframe column that contains the list of sentences to be word-tokenised\n",
    "    \"\"\"\n",
    "    \n",
    "    sents_collector = []\n",
    "    \n",
    "    for answer in answer_col :  \n",
    "        \n",
    "        # no answer was provided -> return empty string list\n",
    "        if not answer:\n",
    "            sents_collector.append(list(\"\"))\n",
    "            \n",
    "        # an answer was provided    \n",
    "        else :\n",
    "            \n",
    "            words_collector = []\n",
    "            \n",
    "            for sent in answer :\n",
    "                \n",
    "                # 1. word-tokenise the answer\n",
    "                words = word_tokenize(sent)\n",
    "                \n",
    "                # 2. convert to lower case\n",
    "                words = [w.lower() for w in words]\n",
    "                \n",
    "                words_collector.append(words)\n",
    "                \n",
    "            sents_collector.append(words_collector)\n",
    "            \n",
    "    return(sents_collector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[str, str, str, str, str, str, str, str, str, str, str, str, str, str]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[type(s) for s in sent_tokenise_answer(cons1_df, idx_Q1)[4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cons1_df.loc[:, 'test1'] = sent_tokenise_answer(cons1_df.iloc[:,idx_Q1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test1_idx = cons1_df.columns.get_loc('test1')      #73"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                   []\n",
       "1                                                   []\n",
       "2                                                   []\n",
       "3    [Moving to a primarily online census: an inevi...\n",
       "4    [A regular full population census is absolutel...\n",
       "Name: test1, dtype: object"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cons1_df.loc[:, 'test1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                   []\n",
       "1                                                   []\n",
       "2                                                   []\n",
       "3    [Moving to a primarily online census: an inevi...\n",
       "4    [A regular full population census is absolutel...\n",
       "Name: test1, dtype: object"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cons1_df.iloc[:, test1_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                                   []\n",
      "1                                                   []\n",
      "2                                                   []\n",
      "3    [Moving to a primarily online census: an inevi...\n",
      "4    [A regular full population census is absolutel...\n",
      "Name: test1, dtype: object\n",
      "[<class 'list'>, <class 'list'>, <class 'list'>, <class 'list'>, <class 'list'>]\n",
      "[[], [], [], [['Moving', 'to', 'a', 'primarily', 'online', 'census', ':', 'an', 'inevitable', 'and', 'necessary', 'evolution', 'of', 'the', 'existing', 'approach', '.'], ['Admin', 'data', 'and', 'surveys', ':', 'an', 'unknown', 'quantity', ',', 'dependent', 'on', 'the', 'quality', 'of', 'admin', 'data', ',', 'and', 'not', 'clear', 'how', 'well', 'it', 'would', 'fulfil', 'the', 'primary', 'aim', 'of', 'a', 'census', ':', 'to', 'produce', 'an', 'accurate', 'and', 'independent', 'estimate', 'of', 'the', 'size', 'and', 'composition', 'of', 'the', 'population', '.']], [['A', 'regular', 'full', 'population', 'census', 'is', 'absolutely', 'necessary', '.'], ['It', 'is', 'the', 'only', 'way', 'to', 'ensure', 'that', 'all', 'population', 'and', 'social', 'statistical', 'estimates', 'and', 'projections', 'are', 'grounded', 'in', 'reality', '.'], ['Data', 'for', 'small', 'areas', 'is', 'absolutely', 'necessary', '.'], ['Without', 'it', ',', 'local', 'authorities', 'would', 'be', 'unable', 'to', 'plan', 'services', ',', 'target', 'resources', 'and', 'measure', 'performance', 'effectively', '.'], ['The', 'cost', 'to', 'the', 'country', 'would', 'exceed', 'the', 'cost', 'of', 'a', 'decennial', 'census', '.'], ['Would', 'suggest', 'that', 'both', 'options', 'should', 'be', 'carried', 'out', ',', 'if', 'possible', ',', 'rather', 'than', 'one', 'or', 'the', 'other', '.'], ['That', 'way', ',', 'we', 'get', 'the', 'accurate', 'picture', 'every', 'ten', 'years', ',', 'and', 'a', 'good/', 'useful', 'indication', 'of', 'trends', 'throughout', 'the', 'in-between', 'periods', '.'], ['If', 'it', 'has', 'to', 'be', 'a', 'choice', 'of', 'just', 'one', 'or', 'the', 'other', ',', 'then', 'a', 'detailed', '10', 'year', 'census', 'would', 'is', 'preferred', '.'], ['The', 'need', 'for', 'accurate', ',', 'precise', 'and', 'regular', 'demographic', 'data', 'is', 'great', 'within', 'large', 'urban', '/', 'metropolitan', 'areas', 'such', 'as', 'Salford', 'and', 'Greater', 'Manchester', 'where', 'the', 'scale', 'and', 'pace', 'of', 'demographic', 'change', 'is', 'substantial', 'and', 'swift', '.'], ['Welfare', 'reform', 'and', 'other', 'changes', 'mean', 'that', 'public', 'services', 'to', 'better', 'identify', 'and', 'target', 'those', 'who', 'are', 'in', 'greatest', 'need', '.'], ['The', 'ability', 'to', 'do', 'that', 'would', 'be', 'severely', 'hampered', 'is', 'detailed', 'small', 'area', 'population', 'data', 'were', 'not', 'available', '.'], ['Output', 'from', 'the', 'decennial', 'census', 'is', 'needed', 'quickly', '.'], ['Suggest', 'that', 'ONS', 'produce', 'fewer', 'research', 'reports', 'in', 'favour', 'of', 'faster', 'data', 'processing', 'and', 'publication', '.'], ['Administrative', 'sources', 'should', 'be', 'used', 'to', 'provide', 'better', 'and', 'more', 'frequent', 'information', 'on', 'deaths', ',', 'school', 'age', 'children', ',', 'sub-national', 'population', 'projections', ',', 'household', 'estimates', 'and', 'projections', 'by', 'household', 'type', '/', 'size', ',']]]\n"
     ]
    }
   ],
   "source": [
    "print(cons1_df.iloc[:, test1_idx]) \n",
    "print([type(a) for a in cons1_df.iloc[:, test1_idx]])\n",
    "print(word_tokenise_answer(cons1_df, test1_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cons1_df.loc[:, 'test2'] = word_tokenise_answer(cons1_df.iloc[:, test1_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test2_idx = cons1_df.columns.get_loc('test2')      #74"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[print(w) for w in cons1_df.iloc[:, test2_idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to calculate polarity score for the answers in our dataset\n",
    "\n",
    "# import key modules\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "from numpy import nan\n",
    "    \n",
    "\n",
    "def get_sentiment_score(answer_col, score_type = 'compound') :\n",
    "    \"\"\" \n",
    "    \n",
    "    Calculate sentiment analysis score (score_type: 'compound' default, 'pos', 'neg')\n",
    "    for the values in the specified dataframe column.\n",
    "    \n",
    "    Return a list of scores, one score for each sentence in the column cell.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # empty list collector of scores\n",
    "    sentiment_bag = []\n",
    "    \n",
    "    for answer in answer_col : \n",
    "        \n",
    "        #print(len(answer))\n",
    "        \n",
    "        # no answer was provided, return NA\n",
    "        if not answer : \n",
    "            sentiment_bag.append(nan)\n",
    "        \n",
    "        # answer is made of only 1 sentence    \n",
    "        elif len(answer) == 1 :\n",
    "            sentiment_bag.append(analyser.polarity_scores(answer)[score_type])\n",
    "        \n",
    "        # answer contains more than one sentence\n",
    "        elif len(answer) > 1 :\n",
    "            sentiment_bag.append([analyser.polarity_scores(s)[score_type] for s in answer])\n",
    "    \n",
    "    return(sentiment_bag)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[nan,\n",
       " nan,\n",
       " nan,\n",
       " [0.0, -0.4585],\n",
       " [0.0,\n",
       "  0.3818,\n",
       "  0.0,\n",
       "  0.4404,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.4404,\n",
       "  0.0,\n",
       "  0.8481,\n",
       "  0.7964,\n",
       "  -0.1779,\n",
       "  0.0,\n",
       "  0.4404,\n",
       "  0.4404]]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "get_sentiment_score(cons1_df.iloc[:, test1_idx], 'compound')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer after sent-tokenisation, before word-tokenisation\n",
    "#[print(list(s)) for s in cons1_df.iloc[3:, test1_idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#[print(str(s)) for s in sent_tokenise_answer(cons1_df, idx_Q1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "nan\n",
      "nan\n",
      "[0.0, -0.4585]\n",
      "[0.0, 0.3818, 0.0, 0.4404, 0.0, 0.0, 0.4404, 0.0, 0.8481, 0.7964, -0.1779, 0.0, 0.4404, 0.4404]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print(result) for result in get_sentiment_score(cons1_df.iloc[:, test1_idx])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan, nan, nan, -0.22925000000000001, 0.25785714285714284]\n",
      "[nan, nan, nan, -0.22925000000000001, 0.19089999999999999]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alessia/anaconda3/lib/python3.6/site-packages/numpy/lib/function_base.py:4016: RuntimeWarning: Invalid value encountered in median\n",
      "  r = func(a, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print([np.mean(np.array(result)) for result in get_sentiment_score(cons1_df.iloc[:,test1_idx])])\n",
    "print([np.median(np.array(result)) for result in get_sentiment_score(cons1_df.iloc[:,test1_idx])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0     0\n",
       "1     0\n",
       "2     0\n",
       "3     2\n",
       "4    14\n",
       "Name: test2, dtype: int64"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#[print(w) for w in cons1_df.iloc[:, test2_idx]];\n",
    "[print(len(w)) for w in cons1_df.iloc[:, test2_idx]]    # count number of senences in each answer\n",
    "cons1_df.iloc[:, test2_idx].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to ....\n",
    "\n",
    "import string\n",
    "\n",
    "def break_words(answer_col, compound_symbol = '-') :\n",
    "    \"\"\"\n",
    "    Break words that are of the compound form word1<symbol>word2 into the constituting words, \n",
    "    then remove empty strings. \n",
    "    \n",
    "    Default compund symbol = '-'\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # empty list collector\n",
    "    tokens_bag = []\n",
    "    \n",
    "    for answer in answer_col :   \n",
    "        \n",
    "        # no answer was provided, return empty string\n",
    "        if not answer : \n",
    "            tokens_bag.append(\"\")\n",
    "            \n",
    "        # an answer was provided       \n",
    "        else :\n",
    "            \n",
    "            print('No. of sentence in the answer = ' + str(len(answer)))\n",
    "            \n",
    "            \n",
    "            # empty collector to make sure we keep the sentences within an answer as separate lists\n",
    "            words_in_s = []\n",
    "            \n",
    "            for sent in answer :\n",
    "                \n",
    "                # empty collector for words within one sentence\n",
    "                words = []\n",
    "                \n",
    "                # 1. break words that are of the form word1-word2 into constituting words\n",
    "            \n",
    "                for w in sent :\n",
    "                \n",
    "                    if compound_symbol in w :\n",
    "                    \n",
    "                        words.extend(w.split(compound_symbol))\n",
    "                    \n",
    "                    else :\n",
    "                    \n",
    "                        words.append(w)\n",
    "                    \n",
    "                    # 2. Remove empty strings\n",
    "                    words = list(filter(None, words))\n",
    "                    \n",
    "                words_in_s.append(words)\n",
    "\n",
    "            tokens_bag.append(words_in_s)\n",
    "    \n",
    "    return(tokens_bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0\n",
       "1     0\n",
       "2     0\n",
       "3     2\n",
       "4    14\n",
       "Name: test2, dtype: int64"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cons1_df.iloc[:, test2_idx].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of sentence in the answer = 2\n",
      "No. of sentence in the answer = 14\n"
     ]
    }
   ],
   "source": [
    "break_words(cons1_df.iloc[:, test2_idx]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = pd.Series([\"\", 'I love double-cream ice-cream', 'I hate fudge. But, I like caramel', 'Pseudo-science. I -care'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I love double-cream ice-cream</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I hate fudge. But, I like caramel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pseudo-science. I -care</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                text\n",
       "0                                   \n",
       "1      I love double-cream ice-cream\n",
       "2  I hate fudge. But, I like caramel\n",
       "3            Pseudo-science. I -care"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_df = pd.DataFrame(text)\n",
    "dummy_df.columns = ['text']\n",
    "dummy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                     \n",
      "1        I love double-cream ice-cream\n",
      "2    I hate fudge. But, I like caramel\n",
      "3              Pseudo-science. I -care\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(dummy_df['text'])\n",
    "dummy_df['text1'] = sent_tokenise_answer(dummy_df.iloc[:, 0])\n",
    "dummy_df['text2'] = word_tokenise_answer(dummy_df.iloc[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of sentence in the answer = 1\n",
      "No. of sentence in the answer = 2\n",
      "No. of sentence in the answer = 2\n"
     ]
    }
   ],
   "source": [
    "dummy_df['text2'] = break_words(dummy_df['text2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                      []\n",
      "1         [I love double-cream ice-cream]\n",
      "2    [I hate fudge., But, I like caramel]\n",
      "3              [Pseudo-science., I -care]\n",
      "Name: text1, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0                                                     \n",
       "1               [[i, love, double, cream, ice, cream]]\n",
       "2    [[i, hate, fudge, .], [but, ,, i, like, caramel]]\n",
       "3                    [[pseudo, science, .], [i, care]]\n",
       "Name: text2, dtype: object"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dummy_df['text1'])\n",
    "dummy_df['text2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define functions to replace contracted negative forms of auxiliary verbs with negation, remove specified stop-words, \n",
    "\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "def fix_neg_aux(answer_col) :\n",
    "    \"\"\"\n",
    "    Replace contracted negative forms of auxiliary verbs with negation (if True).\n",
    "    \n",
    "    Parameters:\n",
    "    - answer_col = dataframe column whose cells contain answer texts\n",
    "    \"\"\"\n",
    "    \n",
    "    # empty list collector for all answers\n",
    "    tokens_bag = []\n",
    "             \n",
    "    for answer in answer_col :   \n",
    "        \n",
    "        \n",
    "        if not answer :             # no answer was provided, return empty string\n",
    "            tokens_bag.append(\"\")\n",
    "            \n",
    "              \n",
    "        else :                      # an answer was provided \n",
    "            \n",
    "            print('No. of sentence in the answer = ' + str(len(answer)))\n",
    "            \n",
    "            # empty collector for individual senences within an asnwer\n",
    "            sep_sents = []\n",
    "            \n",
    "            for sent in answer :\n",
    "                \n",
    "                for w in sent :\n",
    "                        \n",
    "                    if w in [\"don't\", \"didn\", \"didn't\", \"doesn\", \"doesn't\", 'hadn', \n",
    "                             \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \n",
    "                             \"isn't\", 'mightn', \"mightn't\", 'mustn', \"mustn't\", \n",
    "                             'needn', \"needn't\", \"shan't\", 'shouldn', \"shouldn't\", \n",
    "                             'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", \n",
    "                             'wouldn', \"wouldn't\", 'aren', \"aren't\", 'couldn', \"couldn't\"] :\n",
    "                            \n",
    "                        w = 'not'\n",
    "                        \n",
    "                    else :\n",
    "                        \n",
    "                        w = w\n",
    "                        \n",
    "                # collect each sentence as a (separate) list of words\n",
    "                sep_sents.append(new_sent)\n",
    "                \n",
    "            tokens_bag.append(sep_sents)\n",
    "            \n",
    "    return(tokens_bag)\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define functions to replace contracted negative forms of auxiliary verbs with negation, remove specified stop-words, \n",
    "\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "def remove_stopwords(answer_col, stopwords_list=stopwords.words('english'), keep_neg = True) :\n",
    "    \"\"\"\n",
    "    Remove specified stop-words.\n",
    "    \n",
    "    Parameters:\n",
    "    - answer_col = dataframe column whose cells contain answer texts\n",
    "    - keep_neg = whether to remove negation from list of stopwords, (default) True\n",
    "    - stopwords_list = (default) English stopwords from. nltk.corpus\n",
    "    \"\"\"\n",
    "    \n",
    "    # empty list collector for all answers\n",
    "    tokens_bag = []\n",
    "    \n",
    "    \n",
    "    if keep_neg :       # keep negations in\n",
    "        \n",
    "        stopwords_list = [w for w in stopwords_list if not w in ['no', 'nor', 'not', 'only', \n",
    "                                                                 'up', 'down', 'further', \n",
    "                                                                 'too', 'against']]\n",
    "             \n",
    "            \n",
    "    for answer in answer_col :   \n",
    "        \n",
    "        \n",
    "        if not answer :             # no answer was provided, return empty string\n",
    "            tokens_bag.append(\"\")\n",
    "            \n",
    "              \n",
    "        else :                      # an answer was provided \n",
    "            \n",
    "            print('No. of sentence in the answer = ' + str(len(answer)))\n",
    "            \n",
    "            # empty collector for individual senences within an asnwer\n",
    "            sep_sents = []\n",
    "            \n",
    "            for sent in answer :\n",
    "                \n",
    "                # filter out stop words from each answer\n",
    "                new_sent = [w for w in sent if not w in stopwords_list]\n",
    "                \n",
    "                # collect each sentence as a (separate) list of words\n",
    "                sep_sents.append(new_sent)\n",
    "                \n",
    "            tokens_bag.append(sep_sents)\n",
    "            \n",
    "    return(tokens_bag)\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                                     \n",
      "1               [[i, love, double, cream, ice, cream]]\n",
      "2    [[i, hate, fudge, .], [but, ,, i, like, caramel]]\n",
      "3                    [[pseudo, science, .], [i, care]]\n",
      "Name: text2, dtype: object\n",
      "No. of sentence in the answer = 1\n",
      "No. of sentence in the answer = 2\n",
      "No. of sentence in the answer = 2\n",
      "0                                          \n",
      "1       [[love, double, cream, ice, cream]]\n",
      "2    [[hate, fudge, .], [,, like, caramel]]\n",
      "3            [[pseudo, science, .], [care]]\n",
      "Name: text3, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(dummy_df['text2'])\n",
    "dummy_df['text3'] = remove_stopwords(dummy_df['text2'], stopwords_list=stopwords.words('english'))\n",
    "print(dummy_df['text3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text', 'text1', 'text2', 'text3'], dtype='object')"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to part-of-speech tagging sentences\n",
    "\n",
    "from nltk import pos_tag\n",
    "\n",
    "\n",
    "def POS_tagging(answer_col) :\n",
    "    \n",
    "    \"\"\"\n",
    "    Return a list with POS-tags/words tuples for the specified data column.\n",
    "    \n",
    "    Parameters:\n",
    "    - answer_col = dataframe columns containing answer texts, as lists (answers) \n",
    "        of lists (sentences) of tokenised words\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # empty list collector\n",
    "    tokens_bag = []\n",
    "    \n",
    "    for answer in answer_col :   \n",
    "        \n",
    "        # no answer was provided, return empty string\n",
    "        if not answer : \n",
    "            tokens_bag.append(\"\")\n",
    "            \n",
    "        # an answer was provided       \n",
    "        else :\n",
    "            \n",
    "            # empty collector for individual senences within an asnwer\n",
    "            sep_sents = []\n",
    "            \n",
    "            for sent in answer :\n",
    "                \n",
    "                # calculate Part-Of-Speech\n",
    "                pos_answer = pos_tag(sent)\n",
    "                \n",
    "                sep_sents.append(pos_answer)\n",
    "                \n",
    "            \n",
    "            tokens_bag.append(sep_sents)\n",
    "            \n",
    "    return tokens_bag\n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                          \n",
      "1       [[love, double, cream, ice, cream]]\n",
      "2    [[hate, fudge, .], [,, like, caramel]]\n",
      "3            [[pseudo, science, .], [care]]\n",
      "Name: text3, dtype: object\n",
      "['', [[('love', 'NN'), ('double', 'JJ'), ('cream', 'NN'), ('ice', 'NN'), ('cream', 'NN')]], [[('hate', 'NN'), ('fudge', 'NN'), ('.', '.')], [(',', ','), ('like', 'IN'), ('caramel', 'NN')]], [[('pseudo', 'NN'), ('science', 'NN'), ('.', '.')], [('care', 'NN')]]]\n",
      "['', [[('i', 'NN'), ('love', 'VBP'), ('double', 'JJ'), ('cream', 'NN'), ('ice', 'NN'), ('cream', 'NN')]], [[('i', 'JJ'), ('hate', 'NN'), ('fudge', 'NN'), ('.', '.')], [('but', 'CC'), (',', ','), ('i', 'VBP'), ('like', 'IN'), ('caramel', 'NN')]], [[('pseudo', 'NN'), ('science', 'NN'), ('.', '.')], [('i', 'NN'), ('care', 'NN')]]]\n"
     ]
    }
   ],
   "source": [
    "print(dummy_df['text3'])\n",
    "print(POS_tagging(dummy_df['text3']))\n",
    "print(POS_tagging(dummy_df['text2']))\n",
    "#print(dummy_df['text4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[('I', 'PRP'), ('not like', 'VBP'), ('fudge', 'NN')],\n",
       "  [('I', 'PRP'), (\"don't\", 'VBP'), ('like', 'IN'), ('fudge', 'NN')],\n",
       "  [('I', 'PRP'), ('not', 'RB'), ('like', 'IN'), ('fudge', 'NN')]]]"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "POS_tagging([[[\"I\", 'not like', 'fudge'], ['I', \"don't\", 'like', 'fudge'], ['I', 'not', 'like', 'fudge']]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[('I', 'PRP'), ('not buy', 'VBP'), ('fudge', 'NN')],\n",
       "  [('I', 'PRP'), (\"don't\", 'VBP'), ('buy', 'VB'), ('fudge', 'NN')],\n",
       "  [('I', 'PRP'), ('not', 'RB'), ('buy', 'VB'), ('fudge', 'NN')]]]"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "POS_tagging([[[\"I\", 'not buy', 'fudge'], ['I', \"don't\", 'buy', 'fudge'], ['I', 'not', 'buy', 'fudge']]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TBC : should impement something like this...\n",
    "# https://stackoverflow.com/questions/15586721/wordnet-lemmatization-and-pos-tagging-in-python\n",
    "\n",
    "# The following function would map the Peen Treebank tags to WordNet part of speech names:\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    \"\"\"\n",
    "    Return Wordnet POS tags from Penn Treebank tags\n",
    "    \"\"\"\n",
    "    \n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    elif treebank_tag.startswith('S'):\n",
    "        return wordnet.ADJ_SAT\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function \n",
    "\n",
    "# import get_wordnet_pos ?\n",
    "\n",
    "def from_treebank_to_wordnet(POStag_col) :\n",
    "    \n",
    "    # collector for all \n",
    "    lemma_big_bag = []\n",
    "    \n",
    "    \n",
    "    for cell in POStag_col :\n",
    "        \n",
    "        lemma_bag = []\n",
    "        \n",
    "        #print(answer)\n",
    "        #print(len(answer))\n",
    "        \n",
    "        # an answer was provided\n",
    "        if len(cell) > 0 :\n",
    "            \n",
    "            for POStext_pair in cell :\n",
    "                \n",
    "                #print(POStext_pair[0])\n",
    "                #print(POStext_pair[1])\n",
    "                \n",
    "                #print(type(POStext_pair[0]))\n",
    "                #print(type(POStext_pair[1]))\n",
    "                \n",
    "                #print('wordnet pos = ' + get_wordnet_pos(POStext_pair[1]))\n",
    "                #print('type wordnet pos = ' + str(type(get_wordnet_pos(POStext_pair[1]))))\n",
    "                \n",
    "                #print( get_wordnet_pos(POStext_pair[1]) == '')\n",
    "                \n",
    "                \n",
    "                # the treebank POS does not have a wordnet POS equivalent\n",
    "                if get_wordnet_pos(POStext_pair[1]) == '' :\n",
    "                    \n",
    "                    lemma = POStext_pair[0]\n",
    "                    #print('lemma = ' + lemma)\n",
    "                    #print(type(lemma))\n",
    "                    \n",
    "                \n",
    "                \n",
    "                # the treebank POS does have a wordnet POS equivalent\n",
    "                else :\n",
    "                    \n",
    "                    lemma = wordnet_lemmatiser.lemmatize(POStext_pair[0], pos=get_wordnet_pos(POStext_pair[1]))\n",
    "                \n",
    "                    #print('lemma = ' + lemma)\n",
    "                    #print(type(lemma))\n",
    "                    \n",
    "                \n",
    "                lemma_bag.append(lemma)\n",
    "                #print(lemma_bag)\n",
    "                #print(type(lemma_bag))\n",
    "                \n",
    "        else :\n",
    "            \n",
    "            lemma_bag.append(str(\"\"))\n",
    "        \n",
    "        \n",
    "        lemma_big_bag.append(lemma_bag)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
