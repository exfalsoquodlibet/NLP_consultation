{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to sentence-tokenise answers \n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "def sent_tokenise_answer(INPUT) :\n",
    "    \n",
    "    \"\"\" \n",
    "    Function to sentence-tokenise answers. \n",
    "    Return a list of lists with each sublist containing an answer's sentences as strings.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    INPUT : name of the dataframe column containing the list of sentences to be word-tokenised\n",
    "    \"\"\"\n",
    "    \n",
    "    # if no answer was provided -> return empty string list, else sent-tokenize answer\n",
    "    OUTPUT = sent_tokenize(INPUT) if INPUT else list(\"\")\n",
    "            \n",
    "    return pd.Series(dict('sent_tok_text' : OUTPUT))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to word-tokenise sentences \n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def word_tokenise_answer(answer_col) :\n",
    "    \n",
    "    \"\"\" \n",
    "    Function to word-tokenise answers' sentences. \n",
    "    Return a list of lists of lower-case words as strings. \n",
    "    Required input, a list of lists containing sentences as strings.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    answer_col : a list of lists containing the sentences to be word-tokenised\n",
    "    \"\"\"\n",
    "    \n",
    "    sents_collector = []\n",
    "    \n",
    "    for answer in answer_col :  \n",
    "        \n",
    "        # no answer was provided -> return empty string list\n",
    "        if not answer:\n",
    "            sents_collector.append(list(\"\"))\n",
    "            \n",
    "        # an answer was provided    \n",
    "        else :\n",
    "            \n",
    "            # 1. word-tokenise the answer 2. convert to lower case\n",
    "            sents_collector.append([[w.lower() for w in word_tokenize(sent)] for sent in answer])\n",
    "            \n",
    "    return pd.Series(sents_collector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alessia/anaconda3/lib/python3.6/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "# Define function to calculate polarity score for the answers in our dataset\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "from numpy import nan\n",
    "    \n",
    "\n",
    "def get_sentiment_score(answer_col, score_type = 'compound') :\n",
    "    \"\"\" \n",
    "    Calculate sentiment analysis score (score_type: 'compound' default, 'pos', 'neg')\n",
    "    for each sentence in each cell (answer) of the specified dataframe column.\n",
    "    \n",
    "    Return a list of scores, one score for each sentence in the column cell.\n",
    "    If no answer was provided, return NA\n",
    "    \"\"\"\n",
    "    \n",
    "    sentiment_bag = [[analyser.polarity_scores(s)[score_type] for s in answer] for \n",
    "                      answer in answer_col]\n",
    "        \n",
    "    return pd.Series(sentiment_bag)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define function to ....\n",
    "\n",
    "import string\n",
    "\n",
    "def break_words(answer_col, compound_symbol = '-') :\n",
    "    \"\"\"\n",
    "    Break words of the compound form word1<symbol>word2 into the constituting words, \n",
    "    then remove empty strings. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    answer_col : a list of lists containing word-tokenised setences\n",
    "    compound-simbol : compound symbol word1<symbol>word2 to be brokwn as string, default is '-'\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # empty list collector\n",
    "    tokens_bag = []\n",
    "    \n",
    "    for answer in answer_col :   \n",
    "        \n",
    "        # no answer was provided, return empty string\n",
    "        if not answer : \n",
    "            tokens_bag.append(\"\")\n",
    "            \n",
    "        # an answer was provided       \n",
    "        else :\n",
    "            \n",
    "            # empty collector to keep each sentence within an answer as separate list\n",
    "            words_in_s = []\n",
    "            \n",
    "            for sent in answer :\n",
    "                \n",
    "                # empty collector for words within one sentence\n",
    "                words = []\n",
    "                \n",
    "                # 1. break words of the form word1<symbol>word2 into constituting words\n",
    "            \n",
    "                for w in sent :\n",
    "                \n",
    "                    if compound_symbol in w :\n",
    "                    \n",
    "                        words.extend(w.split(compound_symbol))\n",
    "                    \n",
    "                    else :\n",
    "                    \n",
    "                        words.append(w)\n",
    "                    \n",
    "                    # 2. Remove empty strings\n",
    "                    words = list(filter(None, words))\n",
    "                    \n",
    "                words_in_s.append(words)\n",
    "\n",
    "            tokens_bag.append(words_in_s)\n",
    "    \n",
    "    return pd.Series(tokens_bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define functions to replace contracted negative forms of auxiliary verbs with negation, remove specified stop-words, \n",
    "\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "def fix_neg_aux(answer_col) :\n",
    "    \"\"\"\n",
    "    Replace contracted negative forms of auxiliary verbs with negation (if True).\n",
    "    \n",
    "    Parameters:\n",
    "    - answer_col = dataframe column whose cells contain answer texts\n",
    "    \"\"\"\n",
    "    \n",
    "    # empty list collector for all answers\n",
    "    tokens_bag = []\n",
    "             \n",
    "    for answer in answer_col :   \n",
    "        \n",
    "        \n",
    "        if not answer :             # no answer was provided, return empty string\n",
    "            tokens_bag.append(\"\")\n",
    "            \n",
    "              \n",
    "        else :                      # an answer was provided \n",
    "            \n",
    "            sep_sents = []\n",
    "            \n",
    "            for sent in answer :\n",
    "                \n",
    "                new_sent = []\n",
    "                \n",
    "                for w in sent :\n",
    "                        \n",
    "                    if w in [\"don't\", \"didn\", \"didn't\", \"doesn\", \"doesn't\", 'hadn', \"n't\",\n",
    "                             \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \n",
    "                             \"isn't\", 'mightn', \"mightn't\", 'mustn', \"mustn't\", \n",
    "                             'needn', \"needn't\", \"shan't\", 'shouldn', \"shouldn't\", \n",
    "                             'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", \n",
    "                             'wouldn', \"wouldn't\", 'aren', \"aren't\", 'couldn', \"couldn't\"] :\n",
    "                            \n",
    "                        w = 'not'\n",
    "                        \n",
    "                    else :\n",
    "                        \n",
    "                        w = w\n",
    "                        \n",
    "                    new_sent.append(w)\n",
    "                        \n",
    "                # collect each sentence as a (separate) list of words\n",
    "                sep_sents.append(new_sent)\n",
    "                \n",
    "            tokens_bag.append(sep_sents)\n",
    "            \n",
    "    return pd.Series(tokens_bag)\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define functions to replace contracted negative forms of auxiliary verbs with negation, remove specified stop-words, \n",
    "\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "def remove_stopwords(answer_col, stopwords_list=stopwords.words('english'), keep_neg = True) :\n",
    "    \"\"\"\n",
    "    Remove specified stop-words.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    - answer_col : dataframe column whose cells contain answer texts\n",
    "    - keep_neg : whether to remove negation from list of stopwords, (default) True\n",
    "    - stopwords_list : (default) English stopwords from. nltk.corpus\n",
    "    \"\"\"\n",
    "    \n",
    "    # empty list collector for all answers\n",
    "    tokens_bag = []\n",
    "    \n",
    "    \n",
    "    if keep_neg :       # keep negations in\n",
    "        \n",
    "        stopwords_list = [w for w in stopwords_list if not w in ['no', 'nor', 'not', 'only', \n",
    "                                                                 'up', 'down', 'further', \n",
    "                                                                 'too', 'against']]\n",
    "             \n",
    "            \n",
    "    for answer in answer_col :   \n",
    "        \n",
    "        \n",
    "        if not answer :             # no answer was provided, return empty string\n",
    "            tokens_bag.append(\"\")\n",
    "            \n",
    "              \n",
    "        else :                      # an answer was provided \n",
    "            \n",
    "            sep_sents = []\n",
    "            \n",
    "            for sent in answer :\n",
    "                \n",
    "                # filter out stop words from each answer\n",
    "                new_sent = [w for w in sent if not w in stopwords_list]\n",
    "                \n",
    "                # collect each sentence as a (separate) list of words\n",
    "                sep_sents.append(new_sent)\n",
    "                \n",
    "            tokens_bag.append(sep_sents)\n",
    "            \n",
    "    return pd.Series(tokens_bag)\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to part-of-speech tagging sentences\n",
    "\n",
    "from nltk import pos_tag\n",
    "\n",
    "\n",
    "def POS_tagging(answer_col) :\n",
    "    \n",
    "    \"\"\"\n",
    "    Return a list with POS-tags/words tuples for the specified data column.\n",
    "    \n",
    "    Parameters:\n",
    "    - answer_col = dataframe columns containing answer texts, as lists (answers) \n",
    "        of lists (sentences) of tokenised words\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # empty list collector\n",
    "    tokens_bag = []\n",
    "    \n",
    "    for answer in answer_col :   \n",
    "        \n",
    "        # no answer was provided, return empty string\n",
    "        if not answer : \n",
    "            tokens_bag.append(\"\")\n",
    "            \n",
    "        # an answer was provided       \n",
    "        else :\n",
    "            \n",
    "            # empty collector for individual sentences within an asnwer\n",
    "            sep_sents = []\n",
    "            \n",
    "            for sent in answer :\n",
    "                \n",
    "                # calculate Part-Of-Speech\n",
    "                pos_answer = pos_tag(sent)\n",
    "                \n",
    "                sep_sents.append(pos_answer)\n",
    "                \n",
    "            \n",
    "            tokens_bag.append(sep_sents)\n",
    "            \n",
    "    return pd.Series(tokens_bag)\n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TBC : should impement something like this...\n",
    "# https://stackoverflow.com/questions/15586721/wordnet-lemmatization-and-pos-tagging-in-python\n",
    "\n",
    "# The following function would map the Peen Treebank tags to WordNet part of speech names:\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    \"\"\"\n",
    "    Return Wordnet POS tags from Penn Treebank tags\n",
    "    \"\"\"\n",
    "    \n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    elif treebank_tag.startswith('S'):\n",
    "        return wordnet.ADJ_SAT\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function \n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatiser = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "# import get_wordnet_pos ?\n",
    "\n",
    "def lemmatise(POStag_col) :\n",
    "    \n",
    "    \"\"\"\n",
    "    Return lemmas from word-POS tag tuples, using Wordnet POS tags.\n",
    "    When no wornet POS tag is avalable, return the original word.\n",
    "    \n",
    "    Parameters:\n",
    "    - POStag_col = dataframe column containig (word, POS) tuples\n",
    "    \"\"\"\n",
    "    \n",
    "    # collector for all \n",
    "    lemma_big_bag = []\n",
    "    \n",
    "    \n",
    "    for cell in POStag_col :\n",
    "        \n",
    "        #print('No. of sentences (length of cell) = ' + str(len(cell)))\n",
    "        \n",
    "        # an answer was not provided\n",
    "        if len(cell) == 0 :\n",
    "            \n",
    "            lemma_big_bag.append(\"\")\n",
    "            \n",
    "        # an answer was provided\n",
    "        else :\n",
    "            \n",
    "            sent_bag = []\n",
    "            \n",
    "            for sent in cell :\n",
    "                \n",
    "                print('No. of tuples (length of sent) = ' + str(len(sent)))\n",
    "        \n",
    "                lemma_bag = []\n",
    "                \n",
    "                for wordPOS_tuple in sent :\n",
    "                \n",
    "                    # the treebank POS does not have a wordnet POS equivalent \n",
    "                        # -> keep original token\n",
    "                    if get_wordnet_pos(wordPOS_tuple[1]) == '' :\n",
    "                    \n",
    "                        lemma = wordPOS_tuple[0]\n",
    "                    \n",
    "                    # the treebank POS does have a wordnet POS equivalent\n",
    "                    else :\n",
    "                    \n",
    "                        lemma = wordnet_lemmatiser.lemmatize(wordPOS_tuple[0], pos=get_wordnet_pos(wordPOS_tuple[1]))\n",
    "                        \n",
    "                    lemma_bag.append(lemma)\n",
    "                    \n",
    "                sent_bag.append(lemma_bag)\n",
    "                    \n",
    "            lemma_big_bag.append(sent_bag)\n",
    "                    \n",
    "            \n",
    "    return pd.Series(lemma_big_bag)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function\n",
    "\n",
    "def detokenise_sent(word_tokenised_col) :\n",
    "    \n",
    "    \"\"\"\n",
    "    Return a list containing a single string of text for each word-tokenised sentence.\n",
    "    \n",
    "    Parameters:\n",
    "    - word_tokenised_col = dataframe column with word-tokenised sentences\n",
    "    \"\"\"\n",
    "    \n",
    "           \n",
    "    detok_sents = [[\" \".join(sent) for sent in cell] for cell in word_tokenised_col]\n",
    "    \n",
    "    \n",
    "    return(detok_sents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def list2string(list_of_lists) :\n",
    "    \"\"\"\n",
    "    Return a string from a list of strings.\n",
    "    \"\"\"\n",
    "    string_sents = [\" \".join(mylist) for mylist in list_of_lists]\n",
    "\n",
    "    return pd.Series(string_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function\n",
    "import string \n",
    "\n",
    "def remove_punctuation(text_col, item_to_keep = '') :\n",
    "    \n",
    "    \"\"\"\n",
    "    Remove punctuation from a list of strings.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    - text_col : dataframe column with text (each column cell must be a list of sentences as strings)\n",
    "    - item_to_keep : a string of punctuation signs you want to keep in text (e.g., '!?.,:;')\n",
    "    \"\"\"\n",
    "    \n",
    "    # Update string of punctuation signs\n",
    "    if len(item_to_keep) > 0 :\n",
    "        \n",
    "        punctuation_list = ''.join(c for c in string.punctuation if c not in item_to_keep)\n",
    "        \n",
    "    else :\n",
    "        \n",
    "        punctuation_list = string.punctuation\n",
    "        \n",
    "    # Remove punctuation from each word\n",
    "    transtable = str.maketrans('', '', punctuation_list)\n",
    "    \n",
    "    depunct_sent = [[sent.translate(transtable) for sent in cell] for cell in text_col]\n",
    "\n",
    "    return pd.Series(depunct_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
