{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Positive  Negative                                               Text  \\\n",
      "0         0.0       0.7  That our role on the x will become defunct and...   \n",
      "1         0.7       0.1  that the x unit will still have the positive a...   \n",
      "4         0.0       0.6  Lack of respect for staff from the x. It is th...   \n",
      "5         0.7       0.1  best x team I've seen in 30 years of working a...   \n",
      "6         0.0       0.8  The xs are to disconnected from many aspects o...   \n",
      "7         0.0       0.7  One of the speakers \"picking\" on a colleague (...   \n",
      "9         0.0       0.3  That progressing the business overtakes being ...   \n",
      "10        0.0       0.3  learning a not being particularly effective wh...   \n",
      "12        0.0       0.6  I am concerned that putting the survey online ...   \n",
      "14        0.6       0.6  That we will make experts in legacy systems re...   \n",
      "15        0.5       0.5  My biggest concern in X is the escalation proc...   \n",
      "17        0.0       0.5  That there may not be the promised follow thro...   \n",
      "18        0.0       0.3  That these things wont make much difference to...   \n",
      "19        0.0       0.3            Issues with the current Legacy systems    \n",
      "20        0.0       0.4                              Resource and workload   \n",
      "21        0.0       0.4  Not being well-informed on changes that affect...   \n",
      "22        0.0       0.6  The damage to X's reputation if we don't offer...   \n",
      "23        0.2       0.3  There is talk of people being 'the best they c...   \n",
      "24        0.0       0.4  That the skills of existing staff get overlooked.   \n",
      "25        0.0       0.5  That while the vision for what X will look lik...   \n",
      "26        0.0       0.5  That I feel like all this just happens, I don'...   \n",
      "27        0.0       0.6  we fail to deliver on our transformational pie...   \n",
      "28        0.0       0.7  My biggest concern is that more will be expect...   \n",
      "29        0.0       0.7  That x seem to have little or no appetite to a...   \n",
      "30        0.0       0.4   having technology and skills in place for the...   \n",
      "31        0.0       0.5  Our need to spend all the money could continue...   \n",
      "32        0.0       0.5  X not realising what hundreds of staff lower d...   \n",
      "33        0.0       0.6           Too much work! Everything is a priority!   \n",
      "35        0.0       0.0                     I don't have any at the moment   \n",
      "36        0.1       0.3  Realising expected benefits as a result of tra...   \n",
      "..        ...       ...                                                ...   \n",
      "710       0.2       0.0                             putting names to faces   \n",
      "711       0.3       0.0                 Update on the transformation work.   \n",
      "712       0.2       0.0                                    learning update   \n",
      "713       0.3       0.0            Finding out what the plans are for 2017   \n",
      "714       0.4       0.0  The fact there was consistency in the message ...   \n",
      "715       0.4       0.0  X saying that I should write to him regarding ...   \n",
      "717       0.3       0.0            The information on merging data sources   \n",
      "718       0.3       0.0  Hearing about the X future plans and the times...   \n",
      "719       0.4       0.0  Understanding the learning and development the...   \n",
      "720       0.3       0.0          Hearing about the learning opportunities.   \n",
      "722       0.5       0.0  Learning about how the restraints of the diffe...   \n",
      "723       0.6       0.0  Clear up-to-date information concerning curren...   \n",
      "724       0.4       0.0                           The update on the x Camp   \n",
      "725       0.8       0.0  Its good to be able to hear so many of the top...   \n",
      "726       0.6       0.0  Discussion about the potential for sharing dat...   \n",
      "728       0.6       0.0          To be made aware of top-level priorities.   \n",
      "729       0.4       0.0                     A future Learning A for x site   \n",
      "730       0.7       0.0  The question & answer session (there was enoug...   \n",
      "731       0.5       0.0  The answers to the questions - particularly ab...   \n",
      "732       0.6       0.0  All useful - always enjoy hearing from the xs,...   \n",
      "733       0.3       0.0                               data sharing details   \n",
      "734       0.8       0.0  I liked the presenters talking about their asp...   \n",
      "735       0.7       0.0  Being reminded of how valuable the work we do ...   \n",
      "736       0.3       0.0        The statement about the priorities for 2017   \n",
      "737       0.6       0.0  Visibility of key x.  Promoting continuous pro...   \n",
      "738       0.5       0.0  Interesting to know all data policies and stan...   \n",
      "739       0.4       0.0  Information about the way forward regarding co...   \n",
      "741       0.3       0.0                  Information about data acquistion   \n",
      "742       0.6       0.0  Being reminded about the priorities for the of...   \n",
      "743       0.5       0.0  Clear simple statements of what the priority i...   \n",
      "\n",
      "     valence  sentiment  vadersenti  \n",
      "0       0.15          0           0  \n",
      "1       0.80          1           1  \n",
      "4       0.20          0           0  \n",
      "5       0.80          1           1  \n",
      "6       0.10          0           0  \n",
      "7       0.15          0           0  \n",
      "9       0.35          0           1  \n",
      "10      0.35          0           0  \n",
      "12      0.20          0           0  \n",
      "14      0.50          0           0  \n",
      "15      0.50          0           0  \n",
      "17      0.25          0           0  \n",
      "18      0.35          0           0  \n",
      "19      0.35          0           0  \n",
      "20      0.30          0           0  \n",
      "21      0.30          0           0  \n",
      "22      0.20          0           0  \n",
      "23      0.45          0           1  \n",
      "24      0.30          0           0  \n",
      "25      0.25          0           1  \n",
      "26      0.25          0           1  \n",
      "27      0.20          0           0  \n",
      "28      0.15          0           0  \n",
      "29      0.15          0           0  \n",
      "30      0.30          0           0  \n",
      "31      0.25          0           0  \n",
      "32      0.25          0           0  \n",
      "33      0.20          0           0  \n",
      "35      0.50          0           0  \n",
      "36      0.40          0           1  \n",
      "..       ...        ...         ...  \n",
      "710     0.60          1           0  \n",
      "711     0.65          1           0  \n",
      "712     0.60          1           0  \n",
      "713     0.65          1           0  \n",
      "714     0.70          1           1  \n",
      "715     0.70          1           0  \n",
      "717     0.65          1           0  \n",
      "718     0.65          1           0  \n",
      "719     0.70          1           0  \n",
      "720     0.65          1           1  \n",
      "722     0.75          1           1  \n",
      "723     0.80          1           1  \n",
      "724     0.70          1           0  \n",
      "725     0.90          1           1  \n",
      "726     0.80          1           1  \n",
      "728     0.80          1           0  \n",
      "729     0.70          1           0  \n",
      "730     0.85          1           0  \n",
      "731     0.75          1           1  \n",
      "732     0.80          1           1  \n",
      "733     0.65          1           1  \n",
      "734     0.90          1           1  \n",
      "735     0.85          1           0  \n",
      "736     0.65          1           0  \n",
      "737     0.80          1           1  \n",
      "738     0.75          1           1  \n",
      "739     0.70          1           0  \n",
      "741     0.65          1           0  \n",
      "742     0.80          1           0  \n",
      "743     0.75          1           1  \n",
      "\n",
      "[524 rows x 6 columns]\n",
      "accuracyScore: 0.738549618321\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      0.85      0.82       362\n",
      "          1       0.60      0.48      0.53       162\n",
      "\n",
      "avg / total       0.73      0.74      0.73       524\n",
      "\n",
      "            sentiment  vadersenti\n",
      "sentiment    1.000000    0.357618\n",
      "vadersenti   0.357618    1.000000\n",
      "[[309  53]\n",
      " [ 84  78]]\n",
      "Pipeline(steps=[('preprocessor', NLTKPreprocessor(lower=True,\n",
      "         punct={'<', '>', '=', '$', ',', '}', '[', '\\\\', '~', '%', '#', '_', ';', '`', '|', '&', '!', ')', '*', '^', '/', \"'\", ':', '\"', '@', '{', '+', '(', '?', '.', ']', '-'},\n",
      "         stopwords={'haven', 'after', 'nor', 'was', 'been', 'shan', '...timators=10, n_jobs=1, oob_score=False, random_state=None,\n",
      "           verbose=0, warm_start=False))])\n",
      "0.698113207547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mamonu/anaconda3/lib/python3.6/site-packages/sklearn/base.py:122: DeprecationWarning: Estimator NLTKPreprocessor modifies parameters in __init__. This behavior is deprecated as of 0.18 and support for this behavior will be removed in 0.20.\n",
      "  % type(estimator).__name__, DeprecationWarning)\n",
      "/home/mamonu/anaconda3/lib/python3.6/site-packages/sklearn/base.py:122: DeprecationWarning: Estimator NLTKPreprocessor modifies parameters in __init__. This behavior is deprecated as of 0.18 and support for this behavior will be removed in 0.20.\n",
      "  % type(estimator).__name__, DeprecationWarning)\n",
      "/home/mamonu/anaconda3/lib/python3.6/site-packages/sklearn/base.py:122: DeprecationWarning: Estimator NLTKPreprocessor modifies parameters in __init__. This behavior is deprecated as of 0.18 and support for this behavior will be removed in 0.20.\n",
      "  % type(estimator).__name__, DeprecationWarning)\n",
      "/home/mamonu/anaconda3/lib/python3.6/site-packages/sklearn/base.py:122: DeprecationWarning: Estimator NLTKPreprocessor modifies parameters in __init__. This behavior is deprecated as of 0.18 and support for this behavior will be removed in 0.20.\n",
      "  % type(estimator).__name__, DeprecationWarning)\n",
      "/home/mamonu/anaconda3/lib/python3.6/site-packages/sklearn/base.py:122: DeprecationWarning: Estimator NLTKPreprocessor modifies parameters in __init__. This behavior is deprecated as of 0.18 and support for this behavior will be removed in 0.20.\n",
      "  % type(estimator).__name__, DeprecationWarning)\n",
      "/home/mamonu/anaconda3/lib/python3.6/site-packages/sklearn/base.py:122: DeprecationWarning: Estimator NLTKPreprocessor modifies parameters in __init__. This behavior is deprecated as of 0.18 and support for this behavior will be removed in 0.20.\n",
      "  % type(estimator).__name__, DeprecationWarning)\n",
      "/home/mamonu/anaconda3/lib/python3.6/site-packages/sklearn/base.py:122: DeprecationWarning: Estimator NLTKPreprocessor modifies parameters in __init__. This behavior is deprecated as of 0.18 and support for this behavior will be removed in 0.20.\n",
      "  % type(estimator).__name__, DeprecationWarning)\n",
      "/home/mamonu/anaconda3/lib/python3.6/site-packages/sklearn/base.py:122: DeprecationWarning: Estimator NLTKPreprocessor modifies parameters in __init__. This behavior is deprecated as of 0.18 and support for this behavior will be removed in 0.20.\n",
      "  % type(estimator).__name__, DeprecationWarning)\n",
      "/home/mamonu/anaconda3/lib/python3.6/site-packages/sklearn/base.py:122: DeprecationWarning: Estimator NLTKPreprocessor modifies parameters in __init__. This behavior is deprecated as of 0.18 and support for this behavior will be removed in 0.20.\n",
      "  % type(estimator).__name__, DeprecationWarning)\n",
      "/home/mamonu/anaconda3/lib/python3.6/site-packages/sklearn/base.py:122: DeprecationWarning: Estimator NLTKPreprocessor modifies parameters in __init__. This behavior is deprecated as of 0.18 and support for this behavior will be removed in 0.20.\n",
      "  % type(estimator).__name__, DeprecationWarning)\n",
      "/home/mamonu/anaconda3/lib/python3.6/site-packages/sklearn/base.py:122: DeprecationWarning: Estimator NLTKPreprocessor modifies parameters in __init__. This behavior is deprecated as of 0.18 and support for this behavior will be removed in 0.20.\n",
      "  % type(estimator).__name__, DeprecationWarning)\n",
      "/home/mamonu/anaconda3/lib/python3.6/site-packages/sklearn/base.py:122: DeprecationWarning: Estimator NLTKPreprocessor modifies parameters in __init__. This behavior is deprecated as of 0.18 and support for this behavior will be removed in 0.20.\n",
      "  % type(estimator).__name__, DeprecationWarning)\n",
      "/home/mamonu/anaconda3/lib/python3.6/site-packages/sklearn/base.py:122: DeprecationWarning: Estimator NLTKPreprocessor modifies parameters in __init__. This behavior is deprecated as of 0.18 and support for this behavior will be removed in 0.20.\n",
      "  % type(estimator).__name__, DeprecationWarning)\n",
      "/home/mamonu/anaconda3/lib/python3.6/site-packages/sklearn/base.py:122: DeprecationWarning: Estimator NLTKPreprocessor modifies parameters in __init__. This behavior is deprecated as of 0.18 and support for this behavior will be removed in 0.20.\n",
      "  % type(estimator).__name__, DeprecationWarning)\n",
      "/home/mamonu/anaconda3/lib/python3.6/site-packages/sklearn/base.py:122: DeprecationWarning: Estimator NLTKPreprocessor modifies parameters in __init__. This behavior is deprecated as of 0.18 and support for this behavior will be removed in 0.20.\n",
      "  % type(estimator).__name__, DeprecationWarning)\n",
      "/home/mamonu/anaconda3/lib/python3.6/site-packages/sklearn/base.py:122: DeprecationWarning: Estimator NLTKPreprocessor modifies parameters in __init__. This behavior is deprecated as of 0.18 and support for this behavior will be removed in 0.20.\n",
      "  % type(estimator).__name__, DeprecationWarning)\n",
      "/home/mamonu/anaconda3/lib/python3.6/site-packages/sklearn/base.py:122: DeprecationWarning: Estimator NLTKPreprocessor modifies parameters in __init__. This behavior is deprecated as of 0.18 and support for this behavior will be removed in 0.20.\n",
      "  % type(estimator).__name__, DeprecationWarning)\n",
      "/home/mamonu/anaconda3/lib/python3.6/site-packages/sklearn/base.py:122: DeprecationWarning: Estimator NLTKPreprocessor modifies parameters in __init__. This behavior is deprecated as of 0.18 and support for this behavior will be removed in 0.20.\n",
      "  % type(estimator).__name__, DeprecationWarning)\n",
      "/home/mamonu/anaconda3/lib/python3.6/site-packages/sklearn/base.py:122: DeprecationWarning: Estimator NLTKPreprocessor modifies parameters in __init__. This behavior is deprecated as of 0.18 and support for this behavior will be removed in 0.20.\n",
      "  % type(estimator).__name__, DeprecationWarning)\n",
      "/home/mamonu/anaconda3/lib/python3.6/site-packages/sklearn/base.py:122: DeprecationWarning: Estimator NLTKPreprocessor modifies parameters in __init__. This behavior is deprecated as of 0.18 and support for this behavior will be removed in 0.20.\n",
      "  % type(estimator).__name__, DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------NLTK unigram Extra-Trees -------------------------\n",
      "[ 0.75925926  0.85185185  0.90384615  0.86538462  0.82692308  0.82692308\n",
      "  0.5         0.71153846  0.63461538  0.65384615]\n",
      "0.753418803419\n",
      "Pipeline(steps=[('vectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=10000, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "      ...   penalty='l2', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False))])\n",
      "0.792452830189\n",
      "------------unigram-bigram-SGD  Classifier ----------------------------\n",
      "[ 0.72222222  0.75925926  0.86538462  0.90384615  0.96153846  0.84615385\n",
      "  0.65384615  0.86538462  0.84615385  0.78846154]\n",
      "0.821225071225\n",
      "Iteration 1, loss = 0.61012122\n",
      "Validation score: 0.541667\n",
      "Iteration 2, loss = 0.27456970\n",
      "Validation score: 0.541667\n",
      "Iteration 3, loss = 0.15869743\n",
      "Validation score: 0.625000\n",
      "Iteration 4, loss = 0.10904952\n",
      "Validation score: 0.666667\n",
      "Iteration 5, loss = 0.08020205\n",
      "Validation score: 0.708333\n",
      "Iteration 6, loss = 0.06039011\n",
      "Validation score: 0.708333\n",
      "Iteration 7, loss = 0.04686803\n",
      "Validation score: 0.708333\n",
      "Iteration 8, loss = 0.03669043\n",
      "Validation score: 0.708333\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Pipeline(steps=[('vectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=10000, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "      ...e=True, solver='adam', tol=0.0001, validation_fraction=0.05,\n",
      "       verbose=100, warm_start=False))])\n",
      "0.811320754717\n",
      "---------- NeuralNetwork Classifier ---------- \n",
      "Iteration 1, loss = 0.64111384\n",
      "Validation score: 0.833333\n",
      "Iteration 2, loss = 0.29374040\n",
      "Validation score: 0.916667\n",
      "Iteration 3, loss = 0.17149996\n",
      "Validation score: 0.916667\n",
      "Iteration 4, loss = 0.12055621\n",
      "Validation score: 0.875000\n",
      "Iteration 5, loss = 0.08855176\n",
      "Validation score: 0.875000\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.61519231\n",
      "Validation score: 0.666667\n",
      "Iteration 2, loss = 0.32172749\n",
      "Validation score: 0.708333\n",
      "Iteration 3, loss = 0.20521074\n",
      "Validation score: 0.916667\n",
      "Iteration 4, loss = 0.14928729\n",
      "Validation score: 0.916667\n",
      "Iteration 5, loss = 0.11449945\n",
      "Validation score: 0.916667\n",
      "Iteration 6, loss = 0.08915129\n",
      "Validation score: 0.916667\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70293476\n",
      "Validation score: 0.791667\n",
      "Iteration 2, loss = 0.27773849\n",
      "Validation score: 0.875000\n",
      "Iteration 3, loss = 0.15355318\n",
      "Validation score: 0.916667\n",
      "Iteration 4, loss = 0.10555400\n",
      "Validation score: 0.916667\n",
      "Iteration 5, loss = 0.07772716\n",
      "Validation score: 0.875000\n",
      "Iteration 6, loss = 0.05872480\n",
      "Validation score: 0.875000\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.65249742\n",
      "Validation score: 0.708333\n",
      "Iteration 2, loss = 0.35079366\n",
      "Validation score: 0.791667\n",
      "Iteration 3, loss = 0.22459933\n",
      "Validation score: 0.875000\n",
      "Iteration 4, loss = 0.16162920\n",
      "Validation score: 0.833333\n",
      "Iteration 5, loss = 0.12051595\n",
      "Validation score: 0.875000\n",
      "Iteration 6, loss = 0.09229572\n",
      "Validation score: 0.875000\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.65947994\n",
      "Validation score: 0.750000\n",
      "Iteration 2, loss = 0.36850302\n",
      "Validation score: 0.791667\n",
      "Iteration 3, loss = 0.23109190\n",
      "Validation score: 0.875000\n",
      "Iteration 4, loss = 0.16403035\n",
      "Validation score: 0.875000\n",
      "Iteration 5, loss = 0.12190035\n",
      "Validation score: 0.833333\n",
      "Iteration 6, loss = 0.09323477\n",
      "Validation score: 0.791667\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.61491518\n",
      "Validation score: 0.708333\n",
      "Iteration 2, loss = 0.30387356\n",
      "Validation score: 0.750000\n",
      "Iteration 3, loss = 0.18411287\n",
      "Validation score: 0.750000\n",
      "Iteration 4, loss = 0.12964279\n",
      "Validation score: 0.791667\n",
      "Iteration 5, loss = 0.09583077\n",
      "Validation score: 0.875000\n",
      "Iteration 6, loss = 0.07278333\n",
      "Validation score: 0.875000\n",
      "Iteration 7, loss = 0.05593485\n",
      "Validation score: 0.916667\n",
      "Iteration 8, loss = 0.04404661\n",
      "Validation score: 0.916667\n",
      "Iteration 9, loss = 0.03523825\n",
      "Validation score: 0.916667\n",
      "Iteration 10, loss = 0.02909511\n",
      "Validation score: 0.916667\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.57978643\n",
      "Validation score: 0.666667\n",
      "Iteration 2, loss = 0.20878788\n",
      "Validation score: 0.750000\n",
      "Iteration 3, loss = 0.12435699\n",
      "Validation score: 0.875000\n",
      "Iteration 4, loss = 0.08380427\n",
      "Validation score: 0.916667\n",
      "Iteration 5, loss = 0.06103154\n",
      "Validation score: 0.916667\n",
      "Iteration 6, loss = 0.04647135\n",
      "Validation score: 0.916667\n",
      "Iteration 7, loss = 0.03674743\n",
      "Validation score: 0.916667\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.62473876\n",
      "Validation score: 0.708333\n",
      "Iteration 2, loss = 0.31638164\n",
      "Validation score: 0.708333\n",
      "Iteration 3, loss = 0.19656498\n",
      "Validation score: 0.791667\n",
      "Iteration 4, loss = 0.13717098\n",
      "Validation score: 0.791667\n",
      "Iteration 5, loss = 0.10212762\n",
      "Validation score: 0.791667\n",
      "Iteration 6, loss = 0.07761393\n",
      "Validation score: 0.791667\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.68466625\n",
      "Validation score: 0.625000\n",
      "Iteration 2, loss = 0.27965297\n",
      "Validation score: 0.791667\n",
      "Iteration 3, loss = 0.15028199\n",
      "Validation score: 0.791667\n",
      "Iteration 4, loss = 0.10114993\n",
      "Validation score: 0.791667\n",
      "Iteration 5, loss = 0.07405489\n",
      "Validation score: 0.833333\n",
      "Iteration 6, loss = 0.05609110\n",
      "Validation score: 0.875000\n",
      "Iteration 7, loss = 0.04342232\n",
      "Validation score: 0.875000\n",
      "Iteration 8, loss = 0.03437210\n",
      "Validation score: 0.875000\n",
      "Iteration 9, loss = 0.02803482\n",
      "Validation score: 0.916667\n",
      "Iteration 10, loss = 0.02299221\n",
      "Validation score: 0.916667\n",
      "Iteration 11, loss = 0.01960396\n",
      "Validation score: 0.916667\n",
      "Iteration 12, loss = 0.01675241\n",
      "Validation score: 0.916667\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.60329680\n",
      "Validation score: 0.708333\n",
      "Iteration 2, loss = 0.24233267\n",
      "Validation score: 0.875000\n",
      "Iteration 3, loss = 0.14664305\n",
      "Validation score: 0.916667\n",
      "Iteration 4, loss = 0.10587412\n",
      "Validation score: 0.916667\n",
      "Iteration 5, loss = 0.08100245\n",
      "Validation score: 0.916667\n",
      "Iteration 6, loss = 0.06379970\n",
      "Validation score: 0.916667\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[ 0.72222222  0.75925926  0.90384615  0.84615385  0.88461538  0.86538462\n",
      "  0.84615385  0.82692308  0.90384615  0.82692308]\n",
      "0.838532763533\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'eli5'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-0b77e469717b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0meli5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[0meli5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNeuralNetwork_clf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'eli5'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from operator import itemgetter\n",
    "import string\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score,classification_report\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import bigrams\n",
    "from nltk import word_tokenize\n",
    "from nltk.sentiment.util import mark_negation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords as sw\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk import sent_tokenize\n",
    "from nltk import pos_tag\n",
    "from tpot import TPOTClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "\n",
    "\n",
    "# coding=UTF-8\n",
    "\n",
    "from nltk.corpus import brown\n",
    "\n",
    "# This is a fast and simple noun phrase extractor (based on NLTK)\n",
    "# Feel free to use it, just keep a link back to this post\n",
    "# http://thetokenizer.com/2013/05/09/efficient-way-to-extract-the-main-topics-of-a-sentence/\n",
    "# Create by Shlomi Babluki\n",
    "# May, 2013\n",
    "\n",
    "import nltk\n",
    "# This is our fast Part of Speech tagger\n",
    "#############################################################################\n",
    "brown_train = brown.tagged_sents(categories='news')\n",
    "regexp_tagger = nltk.RegexpTagger(\n",
    "    [(r'^-?[0-9]+(.[0-9]+)?$', 'CD'),\n",
    "     (r'(-|:|;)$', ':'),\n",
    "     (r'\\'*$', 'MD'),\n",
    "     (r'(The|the|A|a|An|an)$', 'AT'),\n",
    "     (r'.*able$', 'JJ'),\n",
    "     (r'^[A-Z].*$', 'NNP'),\n",
    "     (r'.*ness$', 'NN'),\n",
    "     (r'.*ly$', 'RB'),\n",
    "     (r'.*s$', 'NNS'),\n",
    "     (r'.*ing$', 'VBG'),\n",
    "     (r'.*ed$', 'VBD'),\n",
    "     (r'.*', 'NN')\n",
    "])\n",
    "unigram_tagger = nltk.UnigramTagger(brown_train, backoff=regexp_tagger)\n",
    "bigram_tagger = nltk.BigramTagger(brown_train, backoff=unigram_tagger)\n",
    "#############################################################################\n",
    "\n",
    "\n",
    "# This is our semi-CFG; Extend it according to your own needs\n",
    "#############################################################################\n",
    "cfg = {}\n",
    "cfg[\"NNP+NNP\"] = \"NNP\"\n",
    "cfg[\"NN+NN\"] = \"NNI\"\n",
    "cfg[\"NNI+NN\"] = \"NNI\"\n",
    "cfg[\"JJ+JJ\"] = \"JJ\"\n",
    "cfg[\"JJ+NN\"] = \"NNI\"\n",
    "#############################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def identity(arg):\n",
    "    \"\"\"\n",
    "    Simple identity function works as a passthrough.\n",
    "    \"\"\"\n",
    "    return arg\n",
    "\n",
    "\n",
    "class NLTKPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transforms input data by using NLTK tokenization, lemmatization, and\n",
    "    other normalization and filtering techniques.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, stopwords=None, punct=None, lower=True, strip=True):\n",
    "        \"\"\"\n",
    "        Instantiates the preprocessor, which make load corpora, models, or do\n",
    "        other time-intenstive NLTK data loading.\n",
    "        \"\"\"\n",
    "        self.lower      = lower\n",
    "        self.strip      = strip\n",
    "        self.stopwords  = set(stopwords) if stopwords else set(sw.words('english'))\n",
    "        self.punct      = set(punct) if punct else set(string.punctuation)\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit simply returns self, no other information is needed.\n",
    "        \"\"\"\n",
    "        return self\n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "        \"\"\"\n",
    "        No inverse transformation\n",
    "        \"\"\"\n",
    "        return X\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Actually runs the preprocessing on each document.\n",
    "        \"\"\"\n",
    "        return [\n",
    "            list(self.tokenize(doc)) for doc in X\n",
    "        ]\n",
    "\n",
    "    def tokenize(self, document):\n",
    "        \"\"\"\n",
    "        Returns a normalized, lemmatized list of tokens from a document by\n",
    "        applying segmentation (breaking into sentences), then word/punctuation\n",
    "        tokenization, and finally part of speech tagging. It uses the part of\n",
    "        speech tags to look up the lemma in WordNet, and returns the lowercase\n",
    "        version of all the words, removing stopwords and punctuation.\n",
    "        \"\"\"\n",
    "        # Break the document into sentences\n",
    "        for sent in sent_tokenize(document):\n",
    "            # Break the sentence into part of speech tagged tokens\n",
    "            for token, tag in pos_tag(wordpunct_tokenize(sent)):\n",
    "                # Apply preprocessing to the token\n",
    "                token = token.lower() if self.lower else token\n",
    "                token = token.strip() if self.strip else token\n",
    "                token = token.strip('_') if self.strip else token\n",
    "                token = token.strip('*') if self.strip else token\n",
    "\n",
    "                # If punctuation or stopword, ignore token and continue\n",
    "                if token in self.stopwords or all(char in self.punct for char in token):\n",
    "                    continue\n",
    "\n",
    "                # Lemmatize the token and yield\n",
    "                lemma = self.lemmatize(token, tag)\n",
    "                yield lemma\n",
    "\n",
    "    def lemmatize(self, token, tag):\n",
    "        \"\"\"\n",
    "        Converts the Penn Treebank tag to a WordNet POS tag, then uses that\n",
    "        tag to perform much more accurate WordNet lemmatization.\n",
    "        \"\"\"\n",
    "        tag = {\n",
    "            'N': wn.NOUN,\n",
    "            'V': wn.VERB,\n",
    "            'R': wn.ADV,\n",
    "            'J': wn.ADJ\n",
    "        }.get(tag[0], wn.NOUN)\n",
    "\n",
    "        return self.lemmatizer.lemmatize(token, tag)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def show_most_informative_features(model, text=None, n=20):\n",
    "    \"\"\"\n",
    "    Accepts a Pipeline with a classifer and a TfidfVectorizer and computes\n",
    "    the n most informative features of the model. If text is given, then will\n",
    "    compute the most informative features for classifying that text.\n",
    "    Note that this function will only work on linear models with coefs_\n",
    "    \"\"\"\n",
    "    # Extract the vectorizer and the classifier from the pipeline\n",
    "    vectorizer = model.named_steps['vectorizer']\n",
    "    classifier = model.named_steps['classifier']\n",
    "\n",
    "    # Check to make sure that we can perform this computation\n",
    "    if not hasattr(classifier, 'coef_'):\n",
    "        raise TypeError(\n",
    "            \"Cannot compute most informative features on {} model.\".format(\n",
    "                classifier.__class__.__name__\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if text is not None:\n",
    "        # Compute the coefficients for the text\n",
    "        tvec = model.transform([text]).toarray()\n",
    "    else:\n",
    "        # Otherwise simply use the coefficients\n",
    "        tvec = classifier.coef_\n",
    "\n",
    "    # Zip the feature names with the coefs and sort\n",
    "    coefs = sorted(\n",
    "        zip(tvec[0], vectorizer.get_feature_names()),\n",
    "        key=itemgetter(0), reverse=True\n",
    "    )\n",
    "\n",
    "    topn  = zip(coefs[:n], coefs[:-(n+1):-1])\n",
    "\n",
    "    # Create the output string to return\n",
    "    output = []\n",
    "\n",
    "    # If text, add the predicted value to the output.\n",
    "    if text is not None:\n",
    "        output.append(\"\\\"{}\\\"\".format(text))\n",
    "        output.append(\"Classified as: {}\".format(model.predict([text])))\n",
    "        output.append(\"\")\n",
    "\n",
    "    # Create two columns with most negative and most positive features.\n",
    "    for (cp, fnp), (cn, fnn) in topn:\n",
    "        output.append(\n",
    "            \"{:0.4f}{: >15}    {:0.4f}{: >15}\".format(cp, fnp, cn, fnn)\n",
    "        )\n",
    "\n",
    "    return \"\\n\".join(output)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.replace(\"<br />\", \" \")\n",
    "    text = text.decode(\"utf-8\")\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def edgemap(input):\n",
    "\n",
    "    if float(input) > 0.5:\n",
    "        sentiment = 1\n",
    "    else:\n",
    "        sentiment = 0\n",
    "\n",
    "    return pd.Series(dict(sentiment=sentiment))\n",
    "\n",
    "\n",
    "def VADERizer(input):\n",
    "    score = vader.polarity_scores(str(input))\n",
    "    compscore = score['compound']\n",
    "\n",
    "\n",
    "    #return pd.Series(dict(vader=1)) if compscore > 0.1 else pd.Series(dict(vader=0))\n",
    "    return pd.Series(dict(vader=1)) if (score['pos'] - 0.15 > score['neg']) else pd.Series(dict(vader=0))\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv('ST.csv',encoding = 'latin1')\n",
    "\n",
    "#delete empty \"-\" lines\n",
    "df = df[df['Text'] != \"-\"]\n",
    "\n",
    "\n",
    "# map from -1 to 1  -> 0 to 1\n",
    "df[\"valence\"] = ((df[\"Positive\"] - df[\"Negative\"])+1)/2.0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print (df.shape) # (25000, 3)\n",
    "# print (df[\"Text\"][0])         # Check out the review\n",
    "# print (df[\"valence\"][0])          # Check out the sentiment (0/1)\n",
    "#\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "df['sentiment'] = df['valence'].apply(lambda x:edgemap(x))\n",
    "df['vadersenti'] = df['Text'].apply(lambda x:VADERizer(x))\n",
    "\n",
    "\n",
    "\n",
    "print(df)\n",
    "\n",
    "\n",
    "\n",
    "print (\"accuracyScore:\",accuracy_score(df['sentiment'], df['vadersenti']))\n",
    "print (classification_report(df['sentiment'], df['vadersenti']))\n",
    "print(df[['sentiment','vadersenti']].corr(method='kendall'))\n",
    "print(confusion_matrix(df['sentiment'],df['vadersenti']))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "experimenting_clf = Pipeline([\n",
    "\n",
    "\n",
    "    ('vectorizer', CountVectorizer(analyzer=\"word\",\n",
    "                                   ngram_range=(1, 2),\n",
    "                                   #tokenizer=word_tokenize,         # ! Comment line to include mark_negation and uncomment next line\n",
    "                                   tokenizer=lambda text: mark_negation(word_tokenize(text)),\n",
    "                                   #preprocessor=lambda text: text.replace(\"<br />\", \" \"),\n",
    "                                   max_features=10000)),\n",
    "\n",
    "    ('classifier', SGDClassifier())\n",
    "\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "unigram_NLTK_clf = Pipeline([\n",
    "\n",
    "\n",
    "    ('preprocessor', NLTKPreprocessor()),\n",
    "\n",
    "    ('vectorizer', TfidfVectorizer(tokenizer=identity, preprocessor=None, lowercase=False)),\n",
    "\n",
    "\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "\n",
    "\n",
    "\n",
    "    ('classifier', ExtraTreesClassifier(bootstrap=False,criterion='gini',max_features=0.75,min_samples_split=13))\n",
    "\n",
    "])\n",
    "\n",
    "\n",
    "NeuralNetwork_clf = Pipeline([\n",
    "\n",
    "\n",
    "    ('vectorizer', CountVectorizer(analyzer=\"word\",\n",
    "                                   ngram_range=(1, 2),\n",
    "                                   #tokenizer=word_tokenize,         # ! Comment line to include mark_negation and uncomment next line\n",
    "                                   tokenizer=lambda text: mark_negation(word_tokenize(text)),\n",
    "                                   #preprocessor=lambda text: text.replace(\"<br />\", \" \"),\n",
    "                                   max_features=10000)),\n",
    "\n",
    "    ('classifier', MLPClassifier(learning_rate_init=0.01,\n",
    "                    hidden_layer_sizes=10, max_iter=100, activation='tanh', verbose=100,\n",
    "                    early_stopping=True, validation_fraction=0.05, alpha=1e-10)\n",
    "\n",
    "     )\n",
    "\n",
    "])\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X=df['Text'].values.astype('U')\n",
    "y=df[\"sentiment\"].values\n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(X,y, test_size=0.1, random_state=0)\n",
    "print(unigram_NLTK_clf.fit(train_X, train_y))\n",
    "print(unigram_NLTK_clf.score(test_X, test_y))\n",
    "\n",
    "\n",
    "\n",
    "# ----------------NLTK unigram Extra-Trees ----------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(unigram_NLTK_clf, X, y, cv=10)\n",
    "\n",
    "print(\"-----------------NLTK unigram Extra-Trees -------------------------\")\n",
    "print(scores)\n",
    "print(np.mean(scores))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(experimenting_clf.fit(train_X, train_y))\n",
    "print(experimenting_clf.score(test_X, test_y))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"------------unigram-bigram-SGD  Classifier ----------------------------\")\n",
    "scores = cross_val_score(experimenting_clf, X, y, cv=10)\n",
    "print (scores)\n",
    "print (np.mean(scores))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#------------------- NeuralNetwork Classifier ----------------------#\n",
    "\n",
    "print(NeuralNetwork_clf.fit(train_X, train_y))\n",
    "print(NeuralNetwork_clf.score(test_X, test_y))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"---------- NeuralNetwork Classifier ---------- \")\n",
    "scores = cross_val_score(NeuralNetwork_clf, X, y, cv=10)\n",
    "print (scores)\n",
    "print (np.mean(scores))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import eli5\n",
    "\n",
    "eli5.show_weights(NeuralNetwork_clf, top=10)\n",
    "eli5.show_weights(experimenting_clf, top=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    table.eli5-weights tr:hover {\n",
       "        filter: brightness(85%);\n",
       "    }\n",
       "</style>\n",
       "\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "        \n",
       "\n",
       "    \n",
       "\n",
       "        \n",
       "            \n",
       "                \n",
       "                \n",
       "    \n",
       "        <p style=\"margin-bottom: 0.5em; margin-top: 0em\">\n",
       "            <b>\n",
       "    \n",
       "        y=1\n",
       "    \n",
       "</b>\n",
       "\n",
       "top features\n",
       "        </p>\n",
       "    \n",
       "    <table class=\"eli5-weights\"\n",
       "           style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto; margin-bottom: 2em;\">\n",
       "        <thead>\n",
       "        <tr style=\"border: none;\">\n",
       "            \n",
       "                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature weights. Note that weights do not account for feature value scales, so if feature values have different scales, features with highest weights might not be the most important.\">\n",
       "                    Weight<sup>?</sup>\n",
       "                </th>\n",
       "            \n",
       "            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
       "            \n",
       "        </tr>\n",
       "        </thead>\n",
       "        <tbody>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 81.29%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +29.815\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        their\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 82.62%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +26.834\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        very\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 82.62%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +26.834\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        great\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 82.62%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +26.834\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        information\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 84.00%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +23.852\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        about\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 84.00%); border: none;\">\n",
       "                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n",
       "                    <i>&hellip; 2201 more positive &hellip;</i>\n",
       "                </td>\n",
       "            </tr>\n",
       "        \n",
       "\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 85.42%); border: none;\">\n",
       "                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n",
       "                    <i>&hellip; 2618 more negative &hellip;</i>\n",
       "                </td>\n",
       "            </tr>\n",
       "        \n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 85.42%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -20.871\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        over\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 84.00%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -23.852\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        like\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 84.00%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -23.852\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        staff\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 81.29%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -29.815\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        will\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 80.00%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -32.797\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        my\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "\n",
       "        </tbody>\n",
       "    </table>\n",
       "\n",
       "            \n",
       "        \n",
       "\n",
       "        \n",
       "\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import eli5\n",
    "\n",
    "eli5.show_weights(NeuralNetwork_clf, top=10)\n",
    "eli5.show_weights(experimenting_clf, top=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
