{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%run 'Improving_functions.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = pd.Series([\"\", \n",
    "                  'I love double-cream ice-cream', \n",
    "                  'I hate fudge! But, I like caramel', \n",
    "                  'Pseudo-science, I -care',\n",
    "                  'You are lying to me: You ate much more than that!',\n",
    "                  \"Why should I have done that?\",\n",
    "                  \"I don't want to hear. Leave me dreaming.\",\n",
    "                  \"Are you still drinking? That's unbelievable! You shouldn't!!!\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                            text\n",
      "0                                                               \n",
      "1  I love double-cream ice-cream                                \n",
      "2  I hate fudge! But, I like caramel                            \n",
      "3  Pseudo-science, I -care                                      \n",
      "4  You are lying to me: You ate much more than that!            \n",
      "5  Why should I have done that?                                 \n",
      "6  I don't want to hear. Leave me dreaming.                     \n",
      "7  Are you still drinking? That's unbelievable! You shouldn't!!!\n"
     ]
    }
   ],
   "source": [
    "dummy_df = pd.DataFrame(text)\n",
    "dummy_df.columns = ['text']\n",
    "print(dummy_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                                                 \n",
      "1    I love double-cream ice-cream                                \n",
      "2    I hate fudge! But, I like caramel                            \n",
      "3    Pseudo-science, I -care                                      \n",
      "4    You are lying to me: You ate much more than that!            \n",
      "5    Why should I have done that?                                 \n",
      "6    I don't want to hear. Leave me dreaming.                     \n",
      "7    Are you still drinking? That's unbelievable! You shouldn't!!!\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(dummy_df['text'])\n",
    "dummy_df['t1_toksen'] = sent_tokenise_answer(dummy_df.iloc[:, 0])\n",
    "dummy_df['t2_tokwor'] = word_tokenise_answer(dummy_df.iloc[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    []                                                                 \n",
      "1    [I love double-cream ice-cream]                                    \n",
      "2    [I hate fudge!, But, I like caramel]                               \n",
      "3    [Pseudo-science, I -care]                                          \n",
      "4    [You are lying to me: You ate much more than that!]                \n",
      "5    [Why should I have done that?]                                     \n",
      "6    [I don't want to hear., Leave me dreaming.]                        \n",
      "7    [Are you still drinking?, That's unbelievable!, You shouldn't!!, !]\n",
      "Name: t1_toksen, dtype: object\n",
      "0    []                                                                                          \n",
      "1    [[i, love, double-cream, ice-cream]]                                                        \n",
      "2    [[i, hate, fudge, !], [but, ,, i, like, caramel]]                                           \n",
      "3    [[pseudo-science, ,, i, -care]]                                                             \n",
      "4    [[you, are, lying, to, me, :, you, ate, much, more, than, that, !]]                         \n",
      "5    [[why, should, i, have, done, that, ?]]                                                     \n",
      "6    [[i, do, n't, want, to, hear, .], [leave, me, dreaming, .]]                                 \n",
      "7    [[are, you, still, drinking, ?], [that, 's, unbelievable, !], [you, should, n't, !, !], [!]]\n",
      "Name: t2_tokwor, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(dummy_df['t1_toksen'])\n",
    "print(dummy_df['t2_tokwor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dummy_df['t3_tokwor_bw'] = break_words(dummy_df['t2_tokwor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                                   []\n",
      "1                      [I love double-cream ice-cream]\n",
      "2                 [I hate fudge!, But, I like caramel]\n",
      "3                            [Pseudo-science, I -care]\n",
      "4    [You are lying to me: You ate much more than t...\n",
      "5                       [Why should I have done that?]\n",
      "6          [I don't want to hear., Leave me dreaming.]\n",
      "7    [Are you still drinking?, That's unbelievable!...\n",
      "Name: t1_toksen, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0                                                     \n",
       "1               [[i, love, double, cream, ice, cream]]\n",
       "2    [[i, hate, fudge, !], [but, ,, i, like, caramel]]\n",
       "3                      [[pseudo, science, ,, i, care]]\n",
       "4    [[you, are, lying, to, me, :, you, ate, much, ...\n",
       "5              [[why, should, i, have, done, that, ?]]\n",
       "6    [[i, do, n't, want, to, hear, .], [leave, me, ...\n",
       "7    [[are, you, still, drinking, ?], [that, 's, un...\n",
       "Name: t3_tokwor_bw, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dummy_df['t1_toksen'])\n",
    "dummy_df['t3_tokwor_bw']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                                     \n",
      "1               [[i, love, double, cream, ice, cream]]\n",
      "2    [[i, hate, fudge, !], [but, ,, i, like, caramel]]\n",
      "3                      [[pseudo, science, ,, i, care]]\n",
      "4    [[you, are, lying, to, me, :, you, ate, much, ...\n",
      "5              [[why, should, i, have, done, that, ?]]\n",
      "6    [[i, do, not, want, to, hear, .], [leave, me, ...\n",
      "7    [[are, you, still, drinking, ?], [that, 's, un...\n",
      "Name: t4_tokwor_fn, dtype: object\n"
     ]
    }
   ],
   "source": [
    "dummy_df['t4_tokwor_fn'] = fix_neg_aux(dummy_df['t3_tokwor_bw'])\n",
    "print(dummy_df['t4_tokwor_fn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                                                   \n",
      "1    [[love, double, cream, ice, cream]]                            \n",
      "2    [[hate, fudge, !], [,, like, caramel]]                         \n",
      "3    [[pseudo, science, ,, care]]                                   \n",
      "4    [[lying, :, ate, much, !]]                                     \n",
      "5    [[done, ?]]                                                    \n",
      "6    [[not, want, hear, .], [leave, dreaming, .]]                   \n",
      "7    [[still, drinking, ?], ['s, unbelievable, !], [not, !, !], [!]]\n",
      "Name: text5_nostopw, dtype: object\n"
     ]
    }
   ],
   "source": [
    "dummy_df['text5_nostopw'] = remove_stopwords(dummy_df['t4_tokwor_fn'], stopwords_list=stopwords.words('english'))\n",
    "print(dummy_df['text5_nostopw'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dummy_df['text6a_pos'] = POS_tagging(dummy_df['t4_tokwor_fn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                                                                                                                                                          \n",
       "1    [[(i, NN), (love, VBP), (double, JJ), (cream, NN), (ice, NN), (cream, NN)]]                                                                                                           \n",
       "2    [[(i, JJ), (hate, VBP), (fudge, NN), (!, .)], [(but, CC), (,, ,), (i, VBP), (like, IN), (caramel, NN)]]                                                                               \n",
       "3    [[(pseudo, NN), (science, NN), (,, ,), (i, NN), (care, NN)]]                                                                                                                          \n",
       "4    [[(you, PRP), (are, VBP), (lying, VBG), (to, TO), (me, PRP), (:, :), (you, PRP), (ate, VBP), (much, RB), (more, JJR), (than, IN), (that, DT), (!, .)]]                                \n",
       "5    [[(why, WRB), (should, MD), (i, VB), (have, VB), (done, VBN), (that, DT), (?, .)]]                                                                                                    \n",
       "6    [[(i, NNS), (do, VBP), (not, RB), (want, VB), (to, TO), (hear, VB), (., .)], [(leave, VB), (me, PRP), (dreaming, VBG), (., .)]]                                                       \n",
       "7    [[(are, VBP), (you, PRP), (still, RB), (drinking, VBG), (?, .)], [(that, DT), ('s, VBZ), (unbelievable, JJ), (!, .)], [(you, PRP), (should, MD), (not, RB), (!, .), (!, .)], [(!, .)]]\n",
       "Name: text6a_pos, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "dummy_df['text6a_pos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_df['text6b_pos'] = POS_tagging(dummy_df['text5_nostopw'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                                                                                            \n",
       "1    [[(love, NN), (double, JJ), (cream, NN), (ice, NN), (cream, NN)]]                                                       \n",
       "2    [[(hate, NN), (fudge, NN), (!, .)], [(,, ,), (like, IN), (caramel, NN)]]                                                \n",
       "3    [[(pseudo, NN), (science, NN), (,, ,), (care, NN)]]                                                                     \n",
       "4    [[(lying, NN), (:, :), (ate, NN), (much, JJ), (!, .)]]                                                                  \n",
       "5    [[(done, VBN), (?, .)]]                                                                                                 \n",
       "6    [[(not, RB), (want, JJ), (hear, NN), (., .)], [(leave, VB), (dreaming, NN), (., .)]]                                    \n",
       "7    [[(still, RB), (drinking, VBG), (?, .)], [('s, POS), (unbelievable, JJ), (!, .)], [(not, RB), (!, .), (!, .)], [(!, .)]]\n",
       "Name: text6b_pos, dtype: object"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "dummy_df['text6b_pos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of tuples (length of sent) = 6\n",
      "No. of tuples (length of sent) = 4\n",
      "No. of tuples (length of sent) = 5\n",
      "No. of tuples (length of sent) = 5\n",
      "No. of tuples (length of sent) = 13\n",
      "No. of tuples (length of sent) = 7\n",
      "No. of tuples (length of sent) = 7\n",
      "No. of tuples (length of sent) = 4\n",
      "No. of tuples (length of sent) = 5\n",
      "No. of tuples (length of sent) = 4\n",
      "No. of tuples (length of sent) = 5\n",
      "No. of tuples (length of sent) = 1\n"
     ]
    }
   ],
   "source": [
    "dummy_df['text7a_lemma'] = lemmatise(dummy_df['text6a_pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                                                            \n",
       "1    [[i, love, double, cream, ice, cream]]                                                  \n",
       "2    [[i, hate, fudge, !], [but, ,, i, like, caramel]]                                       \n",
       "3    [[pseudo, science, ,, i, care]]                                                         \n",
       "4    [[you, be, lie, to, me, :, you, eat, much, more, than, that, !]]                        \n",
       "5    [[why, should, i, have, do, that, ?]]                                                   \n",
       "6    [[i, do, not, want, to, hear, .], [leave, me, dream, .]]                                \n",
       "7    [[be, you, still, drink, ?], [that, 's, unbelievable, !], [you, should, not, !, !], [!]]\n",
       "Name: text7a_lemma, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "dummy_df['text7a_lemma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of tuples (length of sent) = 5\n",
      "No. of tuples (length of sent) = 3\n",
      "No. of tuples (length of sent) = 3\n",
      "No. of tuples (length of sent) = 4\n",
      "No. of tuples (length of sent) = 5\n",
      "No. of tuples (length of sent) = 2\n",
      "No. of tuples (length of sent) = 4\n",
      "No. of tuples (length of sent) = 3\n",
      "No. of tuples (length of sent) = 3\n",
      "No. of tuples (length of sent) = 3\n",
      "No. of tuples (length of sent) = 3\n",
      "No. of tuples (length of sent) = 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0                                                                \n",
       "1    [[love, double, cream, ice, cream]]                         \n",
       "2    [[hate, fudge, !], [,, like, caramel]]                      \n",
       "3    [[pseudo, science, ,, care]]                                \n",
       "4    [[lying, :, ate, much, !]]                                  \n",
       "5    [[do, ?]]                                                   \n",
       "6    [[not, want, hear, .], [leave, dreaming, .]]                \n",
       "7    [[still, drink, ?], ['s, unbelievable, !], [not, !, !], [!]]\n",
       "Name: text7b_lemma, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_df['text7b_lemma'] = lemmatise(dummy_df['text6b_pos'])\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "dummy_df['text7b_lemma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_df['text8a'] = detokenise_sent(dummy_df['text7a_lemma'])\n",
    "dummy_df['text8b'] = detokenise_sent(dummy_df['text7b_lemma'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    []                                                                   \n",
      "1    [i love double cream ice cream]                                      \n",
      "2    [i hate fudge !, but , i like caramel]                               \n",
      "3    [pseudo science , i care]                                            \n",
      "4    [you be lie to me : you eat much more than that !]                   \n",
      "5    [why should i have do that ?]                                        \n",
      "6    [i do not want to hear ., leave me dream .]                          \n",
      "7    [be you still drink ?, that 's unbelievable !, you should not ! !, !]\n",
      "Name: text8a, dtype: object\n",
      "0    []                                            \n",
      "1    [love double cream ice cream]                 \n",
      "2    [hate fudge !, , like caramel]                \n",
      "3    [pseudo science , care]                       \n",
      "4    [lying : ate much !]                          \n",
      "5    [do ?]                                        \n",
      "6    [not want hear ., leave dreaming .]           \n",
      "7    [still drink ?, 's unbelievable !, not ! !, !]\n",
      "Name: text8b, dtype: object\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "print(dummy_df['text8a'])\n",
    "print(dummy_df['text8b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                                                 \n",
      "1    I love double-cream ice-cream                                \n",
      "2    I hate fudge! But, I like caramel                            \n",
      "3    Pseudo-science, I -care                                      \n",
      "4    You are lying to me: You ate much more than that!            \n",
      "5    Why should I have done that?                                 \n",
      "6    I don't want to hear. Leave me dreaming.                     \n",
      "7    Are you still drinking? That's unbelievable! You shouldn't!!!\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(dummy_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "[0.6369]\n",
      "[-0.6114, 0.5023]\n",
      "[0.4939]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[-0.0572, 0.2023]\n",
      "[0.0, 0.2714, 0.0, 0.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print(result) for result in get_sentiment_score(dummy_df['text8a'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "[0.6369]\n",
      "[-0.6114, 0.3612]\n",
      "[0.4939]\n",
      "[-0.5707]\n",
      "[0.0]\n",
      "[-0.0572, -0.0516]\n",
      "[0.0, 0.2714, 0.0, 0.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print(result) for result in get_sentiment_score(dummy_df['text8b'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['x'], ['y', 'z']]\n"
     ]
    }
   ],
   "source": [
    "detoksent1 = detokenise_sent([['x'], [['y'], ['z']]])\n",
    "print(detoksent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def list2string(list_of_lists) :\n",
    "    \"\"\"\n",
    "    Return a string from a list of strings\n",
    "    \"\"\"\n",
    "    string_sents = [\" \".join(mylist) for mylist in list_of_lists]\n",
    "\n",
    "    return(string_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['x']\n",
      "['y', 'z']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['x', 'y z']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list2string(detoksent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cons1_df = pd.read_csv(\"/Users/alessia/Documents/DataScience/NLP_Project/Outputs/cons1_lemmas_df.csv\", nrows=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get column index of questions\n",
    "idx_Q1 = cons1_df.columns.get_loc(str([col for col in cons1_df if 'census methods' in str(col)][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cons1_df.loc[:, 'test1'] = sent_tokenise_answer(cons1_df.iloc[:,idx_Q1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "nan\n",
      "nan\n",
      "[0.0, -0.4585]\n",
      "[0.0, 0.3818, 0.0, 0.4404, 0.0, 0.0, 0.4404, 0.0, 0.8481, 0.7964, -0.1779, 0.0, 0.4404, 0.4404]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print(result) for result in get_sentiment_score(cons1_df.iloc[:, test1_idx])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan, nan, nan, -0.22925000000000001, 0.25785714285714284]\n",
      "[nan, nan, nan, -0.22925000000000001, 0.19089999999999999]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alessia/anaconda3/lib/python3.6/site-packages/numpy/lib/function_base.py:4016: RuntimeWarning: Invalid value encountered in median\n",
      "  r = func(a, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print([np.mean(np.array(result)) for result in get_sentiment_score(cons1_df.iloc[:,test1_idx])])\n",
    "print([np.median(np.array(result)) for result in get_sentiment_score(cons1_df.iloc[:,test1_idx])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
